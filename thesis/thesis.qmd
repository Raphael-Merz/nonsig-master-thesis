---
title: "From Detection to Correction: A Hybrid Rule--NLP Approach to Misinterpretations of Nonsignificant *p* Values"
shorttitle: "Rules and NLP for Correcting p Value Misinterpretations"
author:
  - name: Raphael Merz
    corresponding: true
    orcid: 0000-0002-9474-3379
    email: raphael.merz@rub.de
    affiliations:
      - name: Ruhr University Bochum
        department: Department of Psychology
        city: Bochum
        country: Germany
author-note:
  status-changes: 
    affiliation-change: null
    deceased: null
  disclosures:
    study-registration: null
    data-sharing: null
    related-report: null
    conflict-of-interest: null
    financial-support: null
    gratitude: null
    authorship-agreements: null
abstract: "Misinterpretations of *p* values remain widespread in scientific reporting, despite decades of educational efforts and reform initiatives. One of the most common and consequential errors is interpreting a statistically nonsignificant result (e.g., *p* > .05) as evidence for the absence of an effect — a conclusion not supported by null hypothesis significance testing (NHST). This thesis adopts a human factors perspective, arguing that automation can help mitigate such persistent errors, much like word processors assist with grammar and spelling. I propose an automated, three-step pipeline that detects, classifies, and optionally corrects misinterpretations of nonsignificant results. The evaluation of each of these steps highlights the promise of such an automated approach: In a validation set of 25 articles the automatic detection identified 73% of manually extracted statements. Two easily resolvable issues in the search pattern were found which, once addressed, would increase this reliability to 93%. For classification, three BERT-based models were trained on 930 hand-labeled statements. All models performed well, with the standard BERT model achieving the highest macro F1 score of .92. Finally, the optional correction step proved effective in a validation set 100 statements: 93 statements were correctly phrased after LLM-based revision. These results demonstrate that automation can effectively address this specific misinterpretation and offer a flexible foundation for tackling similar issues in scientific writing and meta-research."
keywords: [p value, misinterpretation, automation, automated checks, RegEx, LLMs, BERT]
floatsintext: true
numbered-lines: false
mask: false # change for blind peer review
bibliography: references.bib
format:
  apaquarto-html: default
  apaquarto-pdf:
    documentmode: man
  apaquarto-typst: default
  apaquarto-docx: default
a4paper: true
---

```{r load libraries}
#| include: false
library(papercheck)
library(readxl)
library(psych)
library(tidyverse)
library(flextable)
library(papaja)
```

# 1. Introduction

Over the past decades, numerous articles have addressed common misinterpretations of *p* values in the context of standard null hypothesis significance testing [NHST\; @goodman08; @greenland_etal16; @schervish96]. Some go further, questioning the use of frequentist methods altogether [@edwards_etal63; @wagenmakers07], while others propose refinements within the frequentist framework that aim to improve the informativeness of statistical inference [@isager_fitzgerald25; @lakens_etal18]. If you are a researcher writing a paper and want to interpret your results correctly, the solution seems simple: read these educational resources and revise your manuscript accordingly. Easy, right? Still, empirical studies consistently show that these misinterpretations remain widespread [@aczel_etal18; @hoekstra_etal06; @murphy_etal25]. So, how might we be able to finally overcome them?

In this article, I show how rule-based approaches, combined with natural language processing (NLP), can be used to automatically detect, classify, and correct these misinterpretations. I focus on the misinterpretation of statistically nonsignificant results as the absence of an effect because it arguably has the strongest impact on researchers’ conclusions and is the most extensively studied misinterpretation of *p* values [@lakens21]. Similarly, my previous work has developed clear criteria for classifying this misinterpretation [@murphy_etal25]. Here, I demonstrate how this automated approach may help us to resolve them.

## 1.1 Misinterpretations and Criticism of *P* Values

The criticism of *p* values has become a prominent and recurring theme in discussions around scientific reform. From claims that they encourage dichotomous thinking [@hoekstra_etal06; @amrhein_etal19] to arguments that they offer little informational value [@wagenmakers07], *p* values – and the broader framework of NHST – have been blamed for many of science’s replication problems [e.g.\; @mcshane_etal19]. On the other hand, many have also argued that NHST per se is not to blame for these problems, but rather how researchers (mis)use and (mis)interpret this tool [e.g., @greenland19; @lakens21]. As a result, many researchers present whole collections of, in their view, common *p* value misinterpretations [see, e.g., @goodman08; @greenland_etal16].

In this study I zoom in on one specific misinterpretation: concluding *no effect* based on a statistically nonsignificant finding. Many studies have previously shown that this misinterpretation remains highly prevalent across time and sub-domains of psychology [@aczel_etal18; @hoekstra_etal06]. In fact, in a recently published article investigating articles published in 2009, 2015, and 2021 across ten different psychology journals, we estimated the prevalence of this misinterpretation in articles’ discussion sections to lie between 76.17% and 84.90% [@murphy_etal25]. These findings highlight that the situation seems not to have greatly improved despite continuous calls to reflect on statistical interpretations of nonsignificant results [e.g., @altman_bland95; @gelman_stern06] and and increasing advocacy for alternative analytical approaches that enable researchers to make informed claims about effects being practically equivalent to zero [e.g., @dienes14; @lakens_etal18].

## 1.2 Possible Solutions

One frequently suggested solution is to improve researchers’ statistical literacy through enhanced education, such as better statistics teaching at the undergraduate and graduate levels [e.g., @lakens21]. However, as noted earlier, the persistent prevalence of the misinterpretation examined in this study indicates that calls for improved education alone might not have been sufficient to address the issue [@murphy_etal25]. This is complemented by research showing that many misinterpretations of *p* values are shared among psychology students and teachers [@badenes-ribera_etal16; @haller_krauss02]. Recognizing the limitations of education alone, researchers have also advocated for the use of alternative analysis techniques within the NHST framework, like equivalence testing or minimum-effect tests [or the combination: three-sided testing\; @isager_fitzgerald25]. These methods allow researchers to test whether an effect is practically relevant and larger than a predefined smallest effect size of interest [SESOI\; @lakens_etal18]. In many contexts, such approaches might be more closely aligned with the substantive questions researchers aim to answer, namely whether an effect is negligible or meaningful in practice.

These proposed solutions also align with the argument made by @lakens21 that *p* value misinterpretations represent a human factors problem, requiring practical and easy-to-implement solutions. In other contexts, we encounter systems like this frequently, be it automatic braking systems in cars, word processors that flag spelling and grammar mistakes, or email clients that filter out malware and phishing attempts. Analogously, automated checks for statistical misinterpretations offer a highly promising route. This perspective emphasizes that many statistical errors arise not from bad intentions or ignorance, but from cognitive limitations and suboptimal workflows.

In the context of research, similar automated solutions are already gaining traction. For instance, the reference manager Zotero flags references to retracted papers [@stillman19]. Statcheck [@nuijten_epskamp24] automatically detects inconsistencies between reported test statistics and *p* values. Other tools, like GRIM, GRIMMER, and SPRITE, identify impossible values in reported summary statistics [@heathers_etal18]. And lastly, Regcheck [@cummin_hussey25] verifies the consistency between manuscripts and their preregistration documents. As AI continues to develop, we can expect these types of automated solutions to become increasingly sophisticated and common.

Following this trend, @R-papercheck developed Papercheck, an R package which allows users to run a battery of automated checks on scientific papers. These include statistical checks (e.g., identifying imprecisely reported *p* values) as well as general manuscript quality checks (e.g., verifying links to online repositories or consistency between in-text citations and reference lists). Papercheck can be used both for single articles (e.g., as writing assistance) and for batches of articles (e.g., for meta-scientific studies). Because this framework is actively maintained and continues to evolve, the approach presented in this study was designed to fit within the Papercheck infrastructure.

# 2. Methods

## 2.1 The Three-Step Pipeline

To provide context for the data used in this study, I first outline the three sub-steps of the proposed pipeline. Statements from scientific articles need to be reliably detected, classified, and finally, if desired, corrected. For each step, I applied specific methods that were best suited to achieve the respective goal.

To detect statements, I used rule-based regular expressions (RegEx) and searched articles' results sections to detect these expressions. Effectively, RegEx searchers are advanced Ctrl+F searches, where a user can include rules like optional characters (e.g., 'significant(ly)' would catch both *significant* and *significantly*) and more complex rules (e.g., 'not.{0,20}significant' allows up to 20 characters between *not* and *significant*). Papercheck has a module that detects most *p* values (see @sec-detection-accuracy in the Results for examples currently not detected) based on RegEx searches. Using this module, I created a subset of all *p* values equal to or above .05.[^1] I then expanded the extracted nonsignificant *p* values to the full sentence and added the preceding and following sentence as context in case of extraction errors (incomplete statements).

[^1]: In the final Papercheck module, users will be able to set the alpha level they used themselves, thus allowing other levels than the conventional 5%.

In the next step, these statements (labeled as correct or incorrect by me; see @sec-statement-classification) were used to train three BERT-based models. BERT (Bidirectional Encoder Representations from Transformers) is a general-purpose language model pre-trained on the BookCorpus and English Wikipedia, making it suitable for a wide range of tasks – but not specifically optimized for scientific language [@devlin_etal19]. Since its introduction, many researchers have developed domain-specific variants of BERT to enhance its performance on specialized tasks. To test whether such domain adaptation improves performance in this study's classification task, I trained two models in addition to standard BERT: SciBERT was trained on a large corpus of scientific articles from Semantic Scholar, particularly in the biomedical and computer science domains [@beltagy_etal19]. PubMedBERT is an even more specific pretrained language model, having been trained exclusively on biomedical abstracts and full-text articles from the PubMed database [@gu_etal22]. In this study, these models were trained and evaluated on their ability to distinguish between correct and incorrect interpretations of nonsignificant results in scientific articles. The models’ hyperparameters (e.g., learning rate, batch size) were informed by established defaults in the field (see [Hugging Face Documentation](https://huggingface.co/docs/transformers/en/main_classes/trainer)) and relevant tutorials [@more25; @talebi24], with further refinements to improve the models' prediction performance.

Lastly, in the application of this framework, statements classified as incorrect by the best-performing BERT model would be sent to a large language model (LLM) for correction. However, to assess how the LLM handles both genuinely incorrect statements and those misclassified as incorrect automatically, I submitted both correct and incorrect statements coded by me to the LLMs in this study.

```{r Papercheck sample library descriptives}
#| include: false
#### Descriptive of the Sample Articles
sample_data <- papercheck::psychsci

submission_years <- c()

# Extract all submission strings
for (article in sample_data) {
  submission <- article[["info"]][["submission"]][[1]]
  submission_years <- c(submission_years, submission)
}

submission_df <- data.frame(submission_years, stringsAsFactors = FALSE)

# Extract the date following "Revision accepted"
submission_df$revision_accepted_date <- sub(
  ".*(?:Revision accepted|Accepted)[ ]*([0-9]{1,2}/[0-9]{1,2}/[0-9]{2}).*",
  "\\1",
  submission_df$submission_years
)

# Clean and normalize dates (pad month/day if needed)
split_dates <- strsplit(submission_df$revision_accepted_date, "/")

normalized_dates <- sapply(split_dates, function(x) {
  if (length(x) == 3) {
    month <- sprintf("%02d", as.numeric(x[1]))
    day   <- sprintf("%02d", as.numeric(x[2]))
    year  <- x[3]
    paste0(month, "/", day, "/", year)
  } else {
    NA
  }
})

# Convert to Date and extract year
submission_df$date_parsed <- as.Date(normalized_dates, format = "%m/%d/%y")
submission_df$publication_year <- format(submission_df$date_parsed, "%Y")

# Convert publication_year to numeric, in case it's still a character
submission_df$publication_year <- as.numeric(submission_df$publication_year)

# Get full descriptive stats
descriptives_article_year <- describe(submission_df$publication_year)

# Extract and round common statistics
median_article_year  <- round(descriptives_article_year$median, 0)
min_article_year     <- round(descriptives_article_year$min, 0)
max_article_year     <- round(descriptives_article_year$max, 0)

# Get Q1 and Q3 using base R
quartiles <- quantile(submission_df$publication_year, probs = c(0.25, 0.75), na.rm = TRUE)
q1_article_year <- round(quartiles[[1]], 0)
q3_article_year <- round(quartiles[[2]], 0)
```

## 2.2 Validation Process and Performance Metrics

To assess the effectiveness of each automated approach, I compared their outputs to human-coded ground truth and calculated appropriate reliability and performance metrics. The validation process was conducted separately for statement detection, classification, and correction.

### 2.2.1 Statement Detection

To ensure that the statement detection process identified all statements with nonsignificant *p* values in articles' results sections, I manually extracted these statements from 25 (10%; randomly chosen) of the Papercheck sample library's 250 open access article from the journal Psychological Science. These articles were published between `r min_article_year` and `r max_article_year`. I then coded whether statements I found were also extracted with the automated RegEx search.

```{r labeled data descriptives}
#| include: false
labeled_data <- read_excel("../data/training_data/labeled/labeled_data.xlsx")

labeled_results <- labeled_data[labeled_data$section == "results",]
labeled_results_99 <- labeled_results[labeled_results$label == -99,]
labeled_results_0 <- labeled_results[labeled_results$label == 0,]
labeled_results_1 <- labeled_results[labeled_results$label == 1,]

keywords_99 <- table(labeled_results_99$label_keywords)

keywords_99_flaseflag <- keywords_99[['false flag']]+keywords_99[['figure note']]
keywords_99_incomplete <- keywords_99[['incomplete']]+keywords_99[['more context needed']]
keywords_99_marginally <- keywords_99[['marginally significant']]+keywords_99[['incorrect other; marginally significant']]
keywords_99_modelfit <- keywords_99[['incomplete']]+keywords_99[['model fit; incomplete']]
```

### 2.2.2 Statement Classification {#sec-statement-classification}

For the training of the BERT models and to assess their final performance, I classified all automatically extracted statements that were detected from an article's results section from Papercheck's sample library. This resulted in `r nrow(labeled_results)` statements in total. Of these, `r nrow(labeled_results_0)` were classified as containing a correct *p* value interpretation by me, and `r nrow(labeled_results_1)` were classified as incorrect *p* value misinterpretations. The remaining `r nrow(labeled_results_99)` statements were classified as neither correct nor incorrect because they interpreted the nonsignificant effect as (marginally) significant (`r keywords_99_marginally`), because the statements were not complete enough to check their correctness (`r keywords_99_incomplete`), because they interpreted model fit indices and not the *p* value (`r keywords_99_modelfit`), because they were falsely flagged as containing a nonsignificant *p* value (e.g., significant *p* values or generic '*p* > .05 indicated by symbol ✝' statements from table/figure notes; `r keywords_99_flaseflag` in total), or due to a combination of these or other reasons (`r nrow(labeled_results_99)-keywords_99_flaseflag-keywords_99_incomplete-keywords_99_marginally-keywords_99_modelfit`).

Before training the model, the labeled data was split into three parts: a test set (20%) for final model evaluation, a training set (72%, or 90% of the remaining 80%) for learning and parameter adjustment, and a validation set (8%, or 10% of the 80%) for calculating evaluation metrics after each epoch (i.e., one full cycle of the model processing the training data) and monitoring overfitting. To address the original class imbalance (with more correct than incorrect statements), the number of correct and incorrect interpretations was balanced in each set, ensuring the model would not simply learn to predict the majority class.

```{r BERT performance}
#| include: false
#################################
#####                       #####
#####     Standard BERT     #####
#####                       #####
#################################

#### Load data
classification_report_BERT <- read.csv("../data/model_performance/classification_report/BERT.csv")
training_history_BERT <- read.csv("../data/model_performance/model_training_history/BERT.csv")
test_predictions_BERT <-read.csv("../data/model_performance/test_predictions/BERT.csv")

#### Data preparation
  # Training History

# Fill NAs in selected columns using the value from the previous row
training_history_BERT <- training_history_BERT |>
  fill(loss, grad_norm, learning_rate, .direction = "down")

# Remove odd-numbered rows
training_history_BERT <- training_history_BERT[seq(2, nrow(training_history_BERT), by = 2), ]

#### Preparing inline citation
  # Classification report
precision_0_BERT <- apa_p(classification_report_BERT$precision[1], 2)
precision_1_BERT <- apa_p(classification_report_BERT$precision[2], 2)
recall_0_BERT <- apa_p(classification_report_BERT$recall[1], 2)
recall_1_BERT <- apa_p(classification_report_BERT$recall[2], 2)
F1_0_BERT <- apa_p(classification_report_BERT$f1.score[1], 2)
F1_1_BERT <- apa_p(classification_report_BERT$f1.score[2], 2)

macro_F1_BERT <- apa_p(classification_report_BERT$f1.score[4], 2)

n_total_BERT <- round(classification_report_BERT$support[5], 0)

  # Training History
n_epochs_BERT <- max(training_history_BERT$epoch)
best_epoch_BERT <- training_history_BERT$epoch[training_history_BERT$eval_loss == min(training_history_BERT$eval_loss)]
  
  # Model Predictions
TP_BERT <- nrow(filter(test_predictions_BERT, test_predictions_BERT$human_label == 1 & test_predictions_BERT$BERT_label == 1))
TN_BERT <- nrow(filter(test_predictions_BERT, test_predictions_BERT$human_label == 0 & test_predictions_BERT$BERT_label == 0))
FP_BERT <- nrow(filter(test_predictions_BERT, test_predictions_BERT$human_label == 0 & test_predictions_BERT$BERT_label == 1))
FN_BERT <- nrow(filter(test_predictions_BERT, test_predictions_BERT$human_label == 1 & test_predictions_BERT$BERT_label == 0))

#################################
#####                       #####
#####        SciBERT        #####
#####                       #####
#################################

#### Load data
classification_report_SciBERT <- read.csv("../data/model_performance/classification_report/SciBERT.csv")
training_history_SciBERT <- read.csv("../data/model_performance/model_training_history/SciBERT.csv")
test_predictions_SciBERT <-read.csv("../data/model_performance/test_predictions/SciBERT.csv")

#### Data preparation
  # Training History

# Fill NAs in selected columns using the value from the previous row
training_history_SciBERT <- training_history_SciBERT |>
  fill(loss, grad_norm, learning_rate, .direction = "down")

# Remove odd-numbered rows
training_history_SciBERT <- training_history_SciBERT[seq(2, nrow(training_history_SciBERT), by = 2), ]

#### Preparing inline citation
  # Classification report
precision_0_SciBERT <- apa_p(classification_report_SciBERT$precision[1], 2)
precision_1_SciBERT <- apa_p(classification_report_SciBERT$precision[2], 2)
recall_0_SciBERT <- apa_p(classification_report_SciBERT$recall[1], 2)
recall_1_SciBERT <- apa_p(classification_report_SciBERT$recall[2], 2)
F1_0_SciBERT <- apa_p(classification_report_SciBERT$f1.score[1], 2)
F1_1_SciBERT <- apa_p(classification_report_SciBERT$f1.score[2], 2)

macro_F1_SciBERT <- apa_p(classification_report_SciBERT$f1.score[4], 2)

n_total_SciBERT <- round(classification_report_SciBERT$support[5], 0)

  # Training History
n_epochs_SciBERT <- max(training_history_SciBERT$epoch)
best_epoch_SciBERT <- training_history_SciBERT$epoch[training_history_SciBERT$eval_loss == min(training_history_SciBERT$eval_loss)]

  # Model Predictions
TP_SciBERT <- nrow(filter(test_predictions_SciBERT, test_predictions_SciBERT$human_label == 1 & test_predictions_SciBERT$BERT_label == 1))
TN_SciBERT <- nrow(filter(test_predictions_SciBERT, test_predictions_SciBERT$human_label == 0 & test_predictions_SciBERT$BERT_label == 0))
FP_SciBERT <- nrow(filter(test_predictions_SciBERT, test_predictions_SciBERT$human_label == 0 & test_predictions_SciBERT$BERT_label == 1))
FN_SciBERT <- nrow(filter(test_predictions_SciBERT, test_predictions_SciBERT$human_label == 1 & test_predictions_SciBERT$BERT_label == 0))

#################################
#####                       #####
#####      PubMedBERT       #####
#####                       #####
#################################

#### Load data
classification_report_PubMedBERT <- read.csv("../data/model_performance/classification_report/PubMedBERT.csv")
training_history_PubMedBERT <- read.csv("../data/model_performance/model_training_history/PubMedBERT.csv")
test_predictions_PubMedBERT <-read.csv("../data/model_performance/test_predictions/PubMedBERT.csv")

#### Data preparation
  # Training History

# Fill NAs in selected columns using the value from the previous row
training_history_PubMedBERT <- training_history_PubMedBERT |>
  fill(loss, grad_norm, learning_rate, .direction = "down")

# Remove odd-numbered rows
training_history_PubMedBERT <- training_history_PubMedBERT[seq(2, nrow(training_history_PubMedBERT), by = 2), ]

#### Preparing inline citation
  # Classification report
precision_0_PubMedBERT <- apa_p(classification_report_PubMedBERT$precision[1], 2)
precision_1_PubMedBERT <- apa_p(classification_report_PubMedBERT$precision[2], 2)
recall_0_PubMedBERT <- apa_p(classification_report_PubMedBERT$recall[1], 2)
recall_1_PubMedBERT <- apa_p(classification_report_PubMedBERT$recall[2], 2)
F1_0_PubMedBERT <- apa_p(classification_report_PubMedBERT$f1.score[1], 2)
F1_1_PubMedBERT <- apa_p(classification_report_PubMedBERT$f1.score[2], 2)

macro_F1_PubMedBERT <- apa_p(classification_report_PubMedBERT$f1.score[4], 2)

n_total_PubMedBERT <- round(classification_report_PubMedBERT$support[5], 0)

  # Training History
n_epochs_PubMedBERT <- max(training_history_PubMedBERT$epoch)
best_epoch_PubMedBERT <- training_history_PubMedBERT$epoch[training_history_PubMedBERT$eval_loss == min(training_history_PubMedBERT$eval_loss)]

  # Model Predictions
TP_PubMedBERT <- nrow(filter(test_predictions_PubMedBERT, test_predictions_PubMedBERT$human_label == 1 & test_predictions_PubMedBERT$BERT_label == 1))
TN_PubMedBERT <- nrow(filter(test_predictions_PubMedBERT, test_predictions_PubMedBERT$human_label == 0 & test_predictions_PubMedBERT$BERT_label == 0))
FP_PubMedBERT <- nrow(filter(test_predictions_PubMedBERT, test_predictions_PubMedBERT$human_label == 0 & test_predictions_PubMedBERT$BERT_label == 1))
FN_PubMedBERT <- nrow(filter(test_predictions_PubMedBERT, test_predictions_PubMedBERT$human_label == 1 & test_predictions_PubMedBERT$BERT_label == 0))
```

During BERT training, I computed the training loss (sum of errors between model predictions and actual labels in the training set) and the validation loss (calculated analogously on the validation set). The best-performing model was selected based on the lowest validation loss to prevent the model from overfitting to the training data. The model would have been trained on a maximum of 16 epochs, but training ended early if the model did not improve, as measured by the validation loss, for two consecutive epochs. Ultimately, the longest number of training epochs was `r max(c(n_epochs_BERT, n_epochs_SciBERT, n_epochs_PubMedBERT))`. For the final evaluation, I computed the fraction of correctly predicted classes among all predicted cases of a class (precision), the fraction of correctly predicted classes among all actual cases of a class (recall), and their harmonic mean (F1 score), separately for each class (incorrect and correct). To summarize overall performance across the two classes, I calculated the unweighted average of the two F1 scores (macro-F1 score).

### 2.2.3 Statement Correction

Lastly, I reviewed 100 statements that were sent to an LLM for correction to evaluate whether the revised statements were correct. Of these, 80 had previously been labeled incorrect and 20 correct by me, allowing me to examine how the LLM would handle false positives from the automated classification. To communicate with the models, I used Papercheck, which relies on the Groq API (available at https://groq.com/). I tested two LLMs – 'llama-3.3-70b-versatile' (created 03-09-2023) and 'openai/gpt-oss-120b' (created 05-08-2025) – and applied two prompts to the full validation dataset of 100 statements. This resulted in three iterations: (1) the initial prompt with the 'llama-3.3-70b-versatile' model, (2) the same prompt with 'openai/gpt-oss-120b', and (3) a refined prompt, developed through preliminary tests on subsets of the 100 statements, with 'openai/gpt-oss-120b'.

Both prompt versions (available on GitHub; see @sec-software) first explained to the LLM that it would receive a statement containing at least one misinterpretation of a nonsignificant finding as the absence of an effect and instructed it to revise only the part of the statement containing this misinterpretation, leaving the rest unchanged. The refined prompt included additional guidance on phrasing to avoid, based on common errors that persisted in revisions from the initial prompt. Finally, the initial prompt instructed the LLM to indicate when it found no nonsignificant *p* value or interpretation of it, to account for possible errors during the automatic detection of statements. This instruction was removed in the refined prompt because the LLM overused this option, which blurred the distinction between statement detection, classification, and correction.

## 2.3 Software {#sec-software}

All scripts for this study were written in R [Version 4.5.0\; @rcoreteam25] or Python [Version 3.12.10\; @python].

In R, I used *papercheck* [Version 0.0.0.9049\; @R-papercheck] for accessing and preprocessing the 250 open access articles, as well as for communication with the LLMs, *readxl* [Version 1.4.5\; @R-readxl] to access Excel files in R, *psych* [Version 2.5.6\; @R-psych] for calculating descriptive statistics, *tidyverse* [Version 2.0.0\; @R-tidyverse] for data preprocessing and visualization, and *flextable* [Version 0.9.9\; @R-flextable], *magick* [Version 2.8.7\; @R-magick], *papaja* [Version 0.1.3\; @R-papaja] and *showtext* [Version 0.9-7\; @R-showtext] to create APA-formatted tables and figures.

All scripts and data to reproduce and use the trained BERT models (Python), analyse the results and validity checks (R and Python), recreate this manuscript (Quarto Markdown in R Studio with the apaquarto extension available at: https://wjschne.github.io/apaquarto/), as well as the list of Python libraries used to train the BERT models are available in the following GitHub repository, together with instructions on how to set it up: LINK.

Due to the project's iterative nature and since no inferential statistical tests were performed, this study was not preregistered. The original project proposal can also be found in the GitHub repository.

```{r detection_check data}
#| include: false

detection_check <- read_excel("../data/detection_check/detection_checked.xlsx")

agreement_table <- table(detection_check$agreement)
agreement_1 <- agreement_table[['1']]
agreement_2 <- agreement_table[['2']]
agreement_3 <- agreement_table[['3']]
agreement_3_FP <- sum(detection_check$agreement == 3 & detection_check$agree_3_FP_FN == "FP", na.rm = TRUE)
agreement_3_FN <- sum(detection_check$agreement == 3 & detection_check$agree_3_FP_FN == "FN", na.rm = TRUE)

keywords_table <- table(detection_check$disagreement_keyword)
keywords_footnote <- keywords_table[['footnote']]
keywords_formatting <- keywords_table[['formatting']]
keywords_hallucination <- keywords_table[['hallucination']]
keywords_character <- keywords_table[['not sure; unusual character']]
keywords_pagebreak <- keywords_table[['page break']]
keywords_ns <- keywords_table[['n.s.']]
keywords_ps <- keywords_table[['p_s']]
```

# 3 Results

## 3.1 Detection Accuracy {#sec-detection-accuracy}

By manually reviewing 25 articles from Papercheck’s sample library, I identified `r agreement_1+agreement_2+agreement_3_FN` statements containing a nonsignificant *p* value. The automated RegEx search fully detected `r agreement_1` (`r round(100*(agreement_1/(agreement_1+agreement_2+agreement_3_FN)))`%) of these, and incompletely detected `r agreement_3_FN` due to extraction errors, often caused by PDF formatting (page breaks, figures, or footnotes). The search also produced `r agreement_3_FP` false positives – statements incorrectly labeled as coming from the results section, but actually originating from other sections, or from table or figure notes. Note, however, that most of the `r agreement_2+agreement_3_FN` (partially) missed statements were due to specific ways of writing (or not writing) the *p* value: `r keywords_ps` were missed because the *p* value was written as '$p_s$' (smallest *p* value in a group of tests), and `r keywords_ns` were missed because the authors wrote 'n.s.' instead of the nonsignificant *p* value. Excluding these two types of reporting, the overall agreement between the automated and manual approaches would have been `r round(100*(agreement_1/(agreement_1+agreement_2+agreement_3_FN-keywords_ns-keywords_ps)))`%.

Lastly, the remaining `r keywords_footnote+keywords_formatting+keywords_hallucination+keywords_character+keywords_pagebreak` missed statements were due to PDF formatting issues such as figures, tables, footnotes, page breaks, or unusual characters within the statement that interfered with the statement detection (`r keywords_footnote+keywords_formatting+keywords_character+keywords_pagebreak` in total).[^2]

[^2]: I could not find one statement that was detected automatically in the article's PDF. My current theory is that this was an artifact from when the PDF was compiled and might be from a different article even, highlighting how impractical the PDF format is in times of increasing automation.

## 3.2 Classification Performance

@fig-loss-curves shows the training and validation loss curves for the three BERT models across their training epochs. The standard BERT model was trained for a total of `r n_epochs_BERT` epochs before early stopping was triggered due to a lack of improvement in validation loss for two consecutive epochs. The model from epoch `r best_epoch_BERT` was therefore selected as the best-performing one. Similarly, SciBERT and PubMedBERT reached the lowest validation loss after epochs `r best_epoch_SciBERT` and `r best_epoch_PubMedBERT`, respectively. As shown in the figure, the training loss consistently decreased throughout training for all three models, as expected since the models were optimized to fit the training data. In contrast, the validation loss plateaued in all models before rising again, indicating that further improvements on the training data no longer translated to better performance on the unseen validation data and may signal the onset of overfitting.

```{r Combine model loss curves into one figure}
#| include: false
library(magick)

# Read all three PDFs (first page of each)
loss_curve_bert     <- image_read_pdf("../data/model_performance/loss_curve/BERT.pdf")
loss_curve_scibert  <- image_read_pdf("../data/model_performance/loss_curve/SciBERT.pdf")
loss_curve_pubmed   <- image_read_pdf("../data/model_performance/loss_curve/PubMedBERT.pdf")

# Combine them horizontally
combined <- image_append(c(loss_curve_bert, loss_curve_scibert, loss_curve_pubmed))

# Save to a new PDF
image_write(combined, path = "../data/model_performance/loss_curve/loss_combined.png", format = "png")
```

![Training and Validation Loss Curves](../data/model_performance/loss_curve/loss_combined.png){#fig-loss-curves width="6in" fig-alt="Curves of the training and validation loss of the three trained BERT models. The best models for standard BERT, SciBERT and PubMedBERT were chosen after epoch `r best_epoch_BERT`, `r best_epoch_SciBERT`, and `r best_epoch_PubMedBERT`, respectively, based on a minmal validation loss." apa-note="Curves of the training and validation loss of the three trained BERT models. The best models for standard BERT, SciBERT and PubMedBERT were chosen after epoch `r best_epoch_BERT`, `r best_epoch_SciBERT`, and `r best_epoch_PubMedBERT`, respectively, based on a minmal validation loss."}

With respect to model performance, @fig-confusion displays the number of correctly and incorrectly classified statements for each model. Overall, all three models performed well, with the standard BERT model showing the fewest misclassifications (`r FP_BERT` false positives and `r FN_BERT` false negatives), while SciBERT and PubMedBERT made `r FP_SciBERT+FN_SciBERT` and `r FP_PubMedBERT+FN_PubMedBERT` false classifications in total, respectively. All models also tended to predict more statements as correct than incorrect, despite the originally balanced class distribution: for example, the standard BERT model classified `r TN_BERT+FN_BERT` statements as correct and `r TP_BERT+FP_BERT` as incorrect, with this difference being even more pronounced in SciBERT (`r TN_SciBERT+FN_SciBERT` vs. `r TP_SciBERT+FP_SciBERT`) and especially PubMedBERT (`r TN_PubMedBERT+FN_PubMedBERT` vs. `r TP_PubMedBERT+FP_PubMedBERT`).

These results are further summarized in @tbl-model-performance. The standard BERT model achieved the highest macro-F1 score (`r macro_F1_BERT`), with SciBERT and PubMedBERT scoring slightly lower (`r macro_F1_SciBERT` and `r macro_F1_PubMedBERT`). Across all models, performance was stronger for predicting correct statements than incorrect ones, as indicated by higher F1 scores and recall in the ‘correct’ class. This pattern, again, underlines that the models tended to overidentify statements as correct rather than incorrect.

```{r Combine model confusion matrices into one figure}
#| include: false
library(magick)

# Read all three PDFs (first page of each)
confusion_matrix_bert     <- image_read_pdf("../data/model_performance/confusion_matrix/BERT.pdf")
confusion_matrix_scibert  <- image_read_pdf("../data/model_performance/confusion_matrix/SciBERT.pdf")
confusion_matrix_pubmed   <- image_read_pdf("../data/model_performance/confusion_matrix/PubMedBERT.pdf")

# Combine them horizontally
combined <- image_append(c(confusion_matrix_bert, confusion_matrix_scibert, confusion_matrix_pubmed))

# Save to a new PDF
image_write(combined, path = "../data/model_performance/confusion_matrix/confusion_matrix_combined.png", format = "png")
```

![Confusion Matrices](../data/model_performance/confusion_matrix/confusion_matrix_combined.png){#fig-confusion width="6.5in" fig-alt="Confusion matrices of the three trained BERT models. Overall, the standard BERT model performed best with a macro-F1 score of `r macro_F1_BERT`." apa-note="Confusion matrices of the three trained BERT models. Overall, the standard BERT model performed best with a macro-F1 score of `r macro_F1_BERT`."}

```{r}
#| label: tbl-model-performance
#| message: false
#| warning: false
#| apa-note: Table of precision, recall and F1 score per model and class.
#| ft-align: left
#| tbl-cap: Model Performance

# Create the data
tbl <- tibble(
  ' '            = c("Correct Class", "Incorrect Class", "Macro F1 score"),
  'BERT 1'       = c(precision_0_BERT, precision_1_BERT, ""),
  'BERT 2'       = c(recall_0_BERT, recall_1_BERT, ""),
  'BERT 3'       = c(F1_0_BERT, F1_1_BERT, macro_F1_BERT),
  'SciBERT 1'    = c(precision_0_SciBERT, precision_1_SciBERT, ""),
  'SciBERT 2'    = c(recall_0_SciBERT, recall_1_SciBERT, ""),
  'SciBERT 3'    = c(F1_0_SciBERT, F1_1_SciBERT, macro_F1_SciBERT),
  'PubMedBERT 1' = c(precision_0_PubMedBERT, precision_1_PubMedBERT, ""),
  'PubMedBERT 2' = c(recall_0_PubMedBERT, recall_1_PubMedBERT, ""),
  'PubMedBERT 3' = c(F1_0_PubMedBERT, F1_1_PubMedBERT, macro_F1_PubMedBERT)
)

# Create the flextable with grouped headers
tbl %>%
  flextable() %>%
  set_header_labels(
    `BERT 1` = "Precision", `BERT 2` = "Recall", `BERT 3` = "F1 score",
    `SciBERT 1` = "Precision", `SciBERT 2` = "Recall", `SciBERT 3` = "F1 score",
    `PubMedBERT 1` = "Precision", `PubMedBERT 2` = "Recall", `PubMedBERT 3` = "F1 score"
  ) %>%
  add_header_row(
  values = c(" ", "BERT", "SciBERT", "PubMedBERT"),
  colwidths = c(1, 3, 3, 3)
  ) %>%
  flextable::fit_to_width(max_width = 6.5) %>%
  fontsize(size = 11, part = "all") %>%
  flextable::theme_apa()
```

@tbl-false-classification shows statements misclassified by all three models to illustrate common sources of difficulty. Potential causes of these misclassifications will be explored in the discussion.

```{r}
#| label: tbl-false-classification
#| message: false
#| warning: false
#| apa-note: Examples for statements that were incorrectly classified by all three BERT models.
#| ft-align: left
#| tbl-cap: Incorrect Model Classifications

# Create the data
tibble(
  'Model Prediction'  = c("False Negative", "False Negative" ,"False Negative", "False Positive"),
  'Statement'         = c(test_predictions_BERT$statement[6], test_predictions_BERT$statement[16], # 6 is a false negative in all models, 
                          test_predictions_BERT$statement[83], test_predictions_BERT$statement[4])) %>%
  flextable() %>%
  width(j = 1, width = 1) %>%   # Set width of column 1 (Error)
  width(j = 2, width = 5) %>%   # Set width of column 1 (Statement)
  #fontsize(size = 11, part = "all") %>%
  flextable::theme_apa()
```

```{r}
#| include: false
#####################
####             ####
#### Version 1.0 ####
####             ####
#####################

llm_corrections_1.0 <- read_excel("../data/llm_correction_check/llm_correction_checked_1.0.xlsx")

n_correct_1.0 <- sum(llm_corrections_1.0$corrected_correct == 0)
n_incorrect_1.0 <- sum(llm_corrections_1.0$corrected_correct == 1)

n_0_correct_1.0 <- sum(llm_corrections_1.0$label == 0 & llm_corrections_1.0$corrected_correct == 0)
n_0_incorrect_1.0 <- sum(llm_corrections_1.0$label == 0 & llm_corrections_1.0$corrected_correct == 1)

n_1_correct_1.0 <- sum(llm_corrections_1.0$label == 1 & llm_corrections_1.0$corrected_correct == 0)
n_1_incorrect_1.0  <- sum(llm_corrections_1.0$label == 1 & llm_corrections_1.0$corrected_correct == 1)

#####################
####             ####
#### Version 1.1 ####
####             ####
#####################

llm_corrections_1.1 <- read_excel("../data/llm_correction_check/llm_correction_checked_1.1.xlsx")

n_correct_1.1 <- sum(llm_corrections_1.1$corrected_correct == 0)
n_incorrect_1.1 <- sum(llm_corrections_1.1$corrected_correct == 1)

n_0_correct_1.1 <- sum(llm_corrections_1.1$label == 0 & llm_corrections_1.1$corrected_correct == 0)
n_0_incorrect_1.1 <- sum(llm_corrections_1.1$label == 0 & llm_corrections_1.1$corrected_correct == 1)

n_1_correct_1.1 <- sum(llm_corrections_1.1$label == 1 & llm_corrections_1.1$corrected_correct == 0)
n_1_incorrect_1.1  <- sum(llm_corrections_1.1$label == 1 & llm_corrections_1.1$corrected_correct == 1)

n_0_not_corrected_1.1 <- sum(llm_corrections_1.1$label == 0 & llm_corrections_1.1$corrected == "NO CORRECTION POSSIBLE")
n_1_not_corrected_1.1 <- sum(llm_corrections_1.1$label == 1 & llm_corrections_1.1$corrected == "NO CORRECTION POSSIBLE")

#####################
####             ####
#### Version 2.0 ####
####             ####
#####################

llm_corrections_2.0 <- read_excel("../data/llm_correction_check/llm_correction_checked_2.0.xlsx")

n_correct_2.0 <- sum(llm_corrections_2.0$corrected_correct == 0)
n_incorrect_2.0 <- sum(llm_corrections_2.0$corrected_correct == 1)

n_0_correct_2.0 <- sum(llm_corrections_2.0$label == 0 & llm_corrections_2.0$corrected_correct == 0)
n_0_incorrect_2.0 <- sum(llm_corrections_2.0$label == 0 & llm_corrections_2.0$corrected_correct == 1)

n_1_correct_2.0 <- sum(llm_corrections_2.0$label == 1 & llm_corrections_2.0$corrected_correct == 0)
n_1_incorrect_2.0  <- sum(llm_corrections_2.0$label == 1 & llm_corrections_2.0$corrected_correct == 1)
```

## 3.3 Correction Evaluation

Of the 100 statements that the LLM was instructed to correct `r n_correct_1.0` were evaluated as correct in the first iteration (initial prompt and older 'llama-3.3-70b-versatile' model). Notably, `r n_0_incorrect_1.0` of the 20 already correct statements were turned incorrect by the LLM, and `r n_1_incorrect_1.0` of the 80 incorrect statements remained incorrect. Using the newer 'openai/gpt-oss-120b' model, `r n_correct_1.1` statements were evaluated as correct after LLM-revision. However, the model left `r n_0_not_corrected_1.1` correct and `r n_1_not_corrected_1.1` incorrect statements unrevised, following instructions in the initial prompt that allowed it to skip statements if it did not find a nonsignificant *p* value or corresponding interpretation. In the final iteration, with the newer model and revised prompt, `r n_correct_2.0` of the 100 LLM-revised statements were correct. Of the remaining `r n_incorrect_2.0` statements, four (one originally correct and three incorrect) described the results as 'compatible with effects of exactly or around zero' – an interpretation that may be technically defensible but still fails to adequately acknowledge the uncertainty in the test result. In one originally correct statement, the LLM removed all *p* values and described the effect as if they were significant. The remaining two were originally incorrect statements, one of which simply remained incorrect, while the other one was altered to the point that it no longer conveyed the original meaning (originally referring to model fit, which was not clear in the revision). Examples of both poor and strong LLM-revisions from this final iteration are shown in @tbl-LLM-corrections-1 and @tbl-LLM-corrections-2, respectively.

```{r}
#| label: tbl-LLM-corrections-1
#| message: false
#| warning: false
#| apa-note: Table of original and LLM-revised statements that were classified as incorrect. In the examples '0' refers to correct and '1' to incorrect.
#| ft-align: left
#| tbl-cap: Examples of Incorrect LLM-Revisions

# Your selected example indices
examples <- c(74, 80)

# Create long format table
df_long <- map_dfr(examples, function(i) {
  example_label <- paste0("From ", llm_corrections_2.0$label[i], " to ", llm_corrections_2.0$corrected_correct[i])
  
  tibble(
    Example = rep(example_label, 2),
    `Statement Type` = c("Original", "LLM-revised"),
    Statement = c(llm_corrections_2.0$statement[i], llm_corrections_2.0$corrected[i]),
    Label = as.character(c(llm_corrections_2.0$label[i], llm_corrections_2.0$corrected_correct[i]))
  )
})

# Create the table with merged 'Example' cells
df_long %>%
  flextable() %>%
  flextable::theme_apa() %>%
  merge_v(j = ~Example) %>%       # Merge vertical cells in the 'Example' column
  valign(j = ~Example, val = "center") %>%  # Align merged cells at the top
  line_spacing(j = c(1:3), space = 1.2) %>%  # Set line spacing in Statement column
  # fontsize(size = 9, part = "all") %>%
  width(j = 1, width = 0.7) %>%   # Set width of column 1 (Example)
  width(j = 2, width = 0.7) %>%   # Column 2 (Statement Type)
  width(j = 3, width = 4) %>%   # Column 3 (Statement)
  width(j = 4, width = 0.7) #%>%   # Column 4 (Label)
  # autofit() %>%
```

```{r}
#| label: tbl-LLM-corrections-2
#| message: false
#| warning: false
#| apa-note: Table of original and LLM-revised statements that were classified as correct. In the examples '0' refers to correct and '1' to incorrect.
#| ft-align: left
#| tbl-cap: Examples of Correct LLM-Revisions

# Your selected example indices
examples <- c(27, 11)

# Fix 'ÃŽÂ²' in statements --> should be beta
llm_corrections_2.0$statement <- gsub("ÃŽÂ²|ÃŽÂ²", "β", llm_corrections_2.0$statement)
llm_corrections_2.0$corrected <- gsub("ÃŽÂ²|ÃŽÂ²", "β", llm_corrections_2.0$corrected)

# Create long format table
df_long <- map_dfr(examples, function(i) {
  example_label <- paste0("From ", llm_corrections_2.0$label[i], " to ", llm_corrections_2.0$corrected_correct[i])
  
  tibble(
    Example = rep(example_label, 2),
    `Statement Type` = c("Original", "LLM-revised"),
    Statement = c(llm_corrections_2.0$statement[i], llm_corrections_2.0$corrected[i]),
    Label = as.character(c(llm_corrections_2.0$label[i], llm_corrections_2.0$corrected_correct[i]))
  )
})

# Create the table with merged 'Example' cells
df_long %>%
  flextable() %>%
  flextable::theme_apa() %>%
  merge_v(j = ~Example) %>%       # Merge vertical cells in the 'Example' column
  valign(j = ~Example, val = "center") %>%  # Align merged cells at the top
  line_spacing(j = c(1:3), space = 1.2) %>%  # Set line spacing in Statement column
  # fontsize(size = 9, part = "all") %>%
  width(j = 1, width = 0.7) %>%   # Set width of column 1 (Example)
  width(j = 2, width = 0.7) %>%   # Column 2 (Statement Type)
  width(j = 3, width = 4) %>%   # Column 3 (Statement)
  width(j = 4, width = 0.7) #%>%   # Column 4 (Label)
  # autofit() %>%
```

# 4 Discussion

## 4.1 Summary of Key Results

In this study, I developed and evaluated a three-step pipeline for automatically detecting, classifying and correcting misinterpretations of nonsignificant results as evidence for the absence of an effect. The pipeline combines (1) rule-based RegEx searches to identify candidate statements, (2) fine-tuned BERT models to classify them as correct or incorrect, and (3) LLMs to generate revised phrasings. Although clear areas for improvement emerged, each component performed well overall. This demonstrates the potential of hybrid, stepwise approaches for addressing persistent reporting issues in scientific writing, with applications beyond *p* value misinterpretations.

Each step contributes distinct strengths. The RegEx-based detection offers a fast, systematic, and transparent way of reducing the volume of text needing NLP-based analysis. The BERT classifiers, having been trained on human-annotated data, provide a reliable and powerful solution for identifying subtle language patterns. The final correction step, while optional, shows that LLMs can generate context-sensitive rewordings that may help authors revise problematic statements.

Moving to the sub-steps, the RegEx-based statement detection phase demonstrated that simple, rule-based searches can effectively flag a large proportion of candidate interpretations. Although formatting issues in PDFs made the correct extraction of these misinterpretations impossible in some cases, the vast majority of statements were automatically detected. In addition, this study revealed straightforward issues in the current approach (e.g., *p* as "p = n.s." or with subscripted '$p_s$') that can be fixed with minimal adjustments, further enhancing the detection accuracy.

The classification results are particularly promising given the relatively small size of the manually labeled dataset (< 1,000 examples, split into training, validation, and test sets). The strong performance likely reflects a certain regularity in how nonsignificance is (mis)interpreted in academic writing – commonly through the use of either 'significant' or 'no effect' (e.g., 'there was no effect', but also 'groups did not differ') terminology. Interestingly, the standard BERT model showed the best overall performance, potentially reflecting that SciBERT’s and PubMedBERT’s more domain-specific knowledge might not have been necessary for the classification of these statements.

Finally, the correction step demonstrated that LLMs can provide useful alternative phrasings to misinterpretations. While further prompt engineering will be necessary to reduce occasional mistakes (especially those turning correct statements into incorrect ones), this step illustrates the potential of integrating generative AI into targeted scientific writing support. Whether such corrections are genuinely valued by authors, however, remains an open question that needs further discussion.

## 4.2 Limitations and Challenges

A key limitation is that the pipeline was tested step by step rather than as a fully integrated system. While detection, classification, and correction each worked well independently, cascading errors are inevitable once these steps are combined. As a result, the overall accuracy in end-to-end use will likely be lower than suggested by isolated evaluations. Nonetheless, any detected misinterpretation should alert authors that their interpretation, some possibly left unflagged, may require reconsideration.

Similarly, the RegEx-based approach proved efficient but also exposed the fragility of rule-based methods when faced with variation in formatting. Cases such as ‘p = n.s.’ or $p_s$ were frequently missed. These errors are relatively easy to address through refinements of the pattern set but highlight that rule-based detection will always struggle with rare or novel notations.

Moving on to classification, the step performed strongly overall, but the use of a single annotator introduces subjectivity into the training labels. While I made efforts to standardize labels – often consulting a statistics expert (my supervisor) on difficult or borderline cases – the classifications ultimately reflect my interpretation of what constitutes a misinterpretation. Ideally, multiple annotators and inter-rater agreement metrics would strengthen the reliability and generalizability of the training dataset. However, the fact that the fine-tuned BERT models generalized well to unseen data suggests that the labeling was systematic enough for the models to learn.

The standard BERT model consistently outperformed the more domain-specific SciBERT and PubMedBERT models, suggesting that this specialization might be hindering in this context. This difference could also be explained by the size of the models’ training datasets: BERT was trained on 3.3 billion words [@devlin_etal19], whereas SciBERT and PubMedBERT were trained on 3.17 billion tokens [words or word fragments\; @beltagy_etal19] and 3.1 billion words [@gu_etal22], respectively. BERT may therefore have achieved slightly better results simply because its broader and larger training data allowed it to represent sentence structure more effectively. Of course, the difference could also stem from numerous other factors in the models’ training configurations. Future research should examine whether this performance gap persists across related tasks.

Similarly, with respect to this study’s training data (articles from Psychological Science published between `r min_article_year` and `r max_article_year`), the findings should not be overgeneralized. While the trained BERT classifiers will likely detect misinterpretations in articles from other journals and time periods as well, the training dataset would nonetheless benefit from being expanded and diversified to cover a broader range of psychological research.

While the standard BERT model performed well overall, it still misclassified edge cases, particularly statements containing both correct and incorrect elements. For example, ‘the difference was not significant, suggesting there is no effect’ combines a factual result with a problematic inference. As can be seen in @tbl-false-classification, all three models often classified such statements wholly as correct. This might be due to the regularity of word choice across classes: correct statements almost always contained the word 'significant(ly)', whereas incorrect statements varied more in their phrasing (e.g., 'no effect/difference', 'groups were the same', 'X did not moderate'). Consequently, once a statement includes significant, the model is heavily nudged toward predicting it as correct, even if it also contains problematic elements. Similarly, if correct statements did not use the word ‘significant’ explicitly, models sometimes misclassified these as incorrect. These issues might be mitigated through more advanced control of the weights the models assign to specific characteristics of a statement. Nevertheless, this still illustrates the difficulty of reducing nuanced writing to a binary label. Splitting statements into parts or using non-binary classification approaches might therefore be useful.

A more practical challenge involves managing models’ tradeoff between false positive and false negative predictions. The current models aim to balance both for optimal macro performance (as seen in the results, this was not perfectly possible). However, in practice, different use cases may prioritize one over the other. For example, an individual researcher using the system to improve their writing may prefer fewer false negatives (i.e., catching as many problematic statements as possible), even at the cost of some false positives. Conversely, a meta-scientist analyzing prevalence trends of this misinterpretation may prioritize precision to avoid overestimating misinterpretations. One way to address this is by allowing users to adjust the model's decision threshold for predicting one label or the other. A future Papercheck module based on this work could incorporate such functionality to fit users’ specific goals.

Lastly, the correction stage revealed both opportunities and challenges. LLMs were able to propose helpful revisions, but, as shown in @tbl-LLM-corrections-1, they also turned already-correct statements incorrect or produced overly generic revisions. Providing them with more context (e.g., the preceding and following sentences or the full paragraph), alongside continued prompt engineering, might improve their ability to generate accurate and nuanced corrections.

However, there is also the flip side to the aforementioned human factors perspective: should authors rely on AI-generated corrections at all? Unlike grammar mistakes that word processors automatically fix for us, misinterpretations of statistical results often stem from deeper conceptual misunderstandings. Automated corrections might inadvertently encourage passivity, shifting responsibility away from researchers’ own critical engagement. In this sense, corrections should, if included at all, be seen as optional guidance, not authoritative replacements for careful reasoning. Alternatively, LLMs could be used to generate explanations of why a given statement is incorrect.

## 4.3 Implications and Future Directions

The pipeline described in this study will be integrated into a new Papercheck module for identifying potential misinterpretations of nonsignificant results. Some clear aspects to improve have been detected in this study: Firstly, the current RegEx searches of Papercheck's ‘all_p_values’ module might not be optimized to detect all different ways in which a *p* values can be written. For example, the previously mentioned $p_s$ is often used to refer to the smallest *p* value in some collection of tests. This is an example of usually irrelevant RegEx's that I will add to improve this automatic detection of candidate statements. Additionally, the dataset used to train the BERT models will also be expanded and re-checked by independent coders to ensure that the aspects the models do pick up are generalizable. Lastly, errors from the LLM-revised corrections will be closely analyzed to inform further prompt engineering and ultimately minimize mistakes.

A broader implication concerns how nonsignificant results should be reported in general. These interpretations are not free-form expressions that authors can adapt to suit their way of writing but rather formalized claims where certain phrasings are demonstrably misleading. Automated systems can support a shift toward clearer and more standardized reporting practices, but ultimately, researchers must recognize their own responsibility in this regard.

This issue is closely connected to the larger question of why such misinterpretations remain highly prevalent despite decades of critique, a phenomenon likely driven by multiple factors. Educational gaps leave many students and researchers uncertain about how to interpret *p* values correctly, as instructors may share the same misconceptions [@haller_krauss02] or textbooks may themselves misrepresent key statistical concepts [@cassidy_etal19]. Similarly, researchers might have different philosophies of science and might disagree about how to interpret key statistics like the *p* value [@lakens21]. Finally, the widespread prevalence of misinterpretations itself might create a self-reinforcing cycle, with researchers adopting the language they encounter in published articles and thereby perpetuating the problem. Automated feedback systems cannot resolve these deeper causes, but they may, for now, assist individual authors by highlighting such mistakes in their own work.

Additionally, the pipeline’s stepwise structure makes it easy to adapt to other classification or correction tasks than the one presented in this study. For instance, users could train custom classifiers to detect different issues in reporting practices [see @vanabkoude25 for an application to problematic causal language]. In practice, this would involve specifying RegEx patterns that capture the target aspects, training classifiers to label them as correct or incorrect, and, if desired, creating a prompt to generate corrections. Depending on the issue at hand, such classifiers could also be trained on existing hand-labeled datasets from meta-scientific studies where researchers coded specific practices or mistakes [e.g., @aczel_etal18].

Finally, the most important next step for this project is to conduct qualitative user studies to explore how authors would prefer a tool like this to be designed and implemented. A central question will again concern the role of the optional correction feature: do authors find value in receiving suggested corrections, or is simple flagging sufficient? These studies could also reveal where customization is most useful (e.g., varying levels of strictness, setting a personal alpha level instead of the conventional 5%, etc.). In addition, experimental evaluations would help assess whether the tool reduces the prevalence of misinterpretations and increases authors’ awareness of them.

# 5 Conclusion

This study demonstrates that a hybrid rule-based and NLP-driven pipeline can effectively detect, classify, and correct the common statistical misinterpretation that nonsignificant results are evidence for the absence of an effect. Each of these steps performed well independently. The next step is to evaluate the pipeline as a fully automated system in real-world use cases and conduct qualitative user studies to inform the tool's implementation. With further refinement, automated manuscript checks like the one presented in this article could substantially improve the accuracy of scientific reporting, while also providing a valuable resource for large-scale meta-scientific analyses.

# Achknowledgement

I want to thank Dr. Daniël Lakens for his constant support throughout this thesis and for a wonderful research stay that allowed me to work on it in person. I thank Solveig Steland for her unwavering support, for encouraging me when I felt discouraged, and for very insightful discussions on whether and how we (should and) should not use AI. I also thank Prof. Dr. Maike Luhmann for allowing me to pursue a meta-scientific project that is so close to my heart, even though it falls somewhat outside her area of expertise. Finally, I thank Christian Sodano for helpful discussions about machine learning and BERT models, which helped to ensure that my model training did not turn into 'algorithmic *p*-hacking'.
 

# References

<!-- References will auto-populate in the refs div below -->

::: {#refs}
:::

