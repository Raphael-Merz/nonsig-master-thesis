---
title: "From Detection to Correction: A Hybrid NLP Approach to Misinterpretations of Nonsignificant *p* Values"
shorttitle: "Hybrid NLP for Correcting p Value Misinterpretations"
author:
  - name: Raphael Merz
    corresponding: true
    orcid: 0000-0002-9474-3379
    email: raphael.merz@rub.de
    affiliations:
      - name: Ruhr University Bochum
        department: Department of Psychology
        city: Bochum
        country: Germany
author-note:
  status-changes: 
    affiliation-change: null
    deceased: null
  disclosures:
    study-registration: null
    data-sharing: null
    related-report: null
    conflict-of-interest: null
    financial-support: null
    gratitude: null
    authorship-agreements: null
abstract: "Misinterpretations of *p* values remain widespread in scientific reporting, despite decades of educational efforts and reform initiatives. One of the most common and consequential errors is interpreting a statistically nonsignificant result (e.g., *p* > .05) as evidence for the absence of an effect—a conclusion not supported by null hypothesis significance testing (NHST). This thesis adopts a human factors perspective, arguing that automation can help mitigate such persistent errors, much like word processors assist with grammar and spelling. I propose an automated, three-step pipeline that detects, classifies, and optionally corrects misinterpretations of nonsignificant results. Evaluation of each step highlights the promise of such an automated approach: In a validation set of 25 articles the automatic detection identified 73% of human-extracted statements. Two easily resolvable issues in the search pattern were found which, once addressed, would increase this reliability to 93%. For classification, three BERT-based models were trained on 930 hand-labeled statements. All performed well, with SciBERT achieving the highest macro F1 score of .91. Finally, the optional correction step proved effective in a validation set of 80 incorrect and 20 correcct statements: 85 statements were correctly phrased after LLM-based revision. These results demonstrate that automation can effectively address this specific misinterpretation and offer a flexible foundation for tackling similar issues in scientific writing and meta-research."
keywords: [p value, misinterpretation, automation, automated checks, RegEx, LLMs, BERT]
floatsintext: true
numbered-lines: false
mask: false # change for blind peer review
bibliography: references.bib
format:
  apaquarto-html: default
  apaquarto-pdf:
    documentmode: man
  apaquarto-typst: default
  apaquarto-docx: default
a4paper: true
---

```{r load libraries}
#| include: false
library(papercheck)
library(readxl)
library(psych)
library(tidyverse)
library(flextable)
library(papaja)
```

# 1 Introduction

Over the past decades, numerous articles have addressed common misinterpretations of *p* values in the context of standard null hypothesis significance testing (NHST) [@goodman08; @greenland_etal16; @schervish96]. Some go further, questioning the use of frequentist methods altogether [@edwards_etal63; @wagenmakers07], while others propose refinements within the frequentist framework that aim to improve the informativeness of statistical inference [@isager_fitzgerald25; @lakens_etal18]. If you are a researcher writing a paper and want to interpret your results correctly, the solution seems simple: read these educational resources and revise your manuscript accordingly. Easy, right? Still, empirical studies consistently show that these misinterpretations remain widespread [@hoekstra_etal06; @murphy_etal25]. Why is that? What makes interpreting *p* values so persistently difficult? And which practical solutions or promising approaches might help?

In this article, I show how rule-based approaches, combined with natural language processing (NLP), can be used to automatically detect, classify, and correct these misinterpretations. I focus on the misinterpretation of statistically nonsignificant results as the absence of an effect because it arguably has the strongest impact on researchers’ conclusions and is the most extensively studied misinterpretation of *p* values [@lakens21]. Similarly, my previous work has developed clear criteria for classifying this misinterpretation [@murphy_etal25]. I demonstrate how this automated approach may help us to finally overcome this misinterpretation.

## 1.1 Misinterpretations and Criticism of *P* Values

The criticism of *p* values has become a prominent and recurring theme in discussions around scientific reform. From claims that they encourage dichotomous thinking [@hoekstra_etal06; @amrhein_etal19] to arguments that they offer little informational value [@wagenmakers07], *p* values – and the broader framework of NHST – have been blamed for many of science’s replication problems [@mcshane_etal19]. On the other hand, many have also argued that NHST per se is not to blame for these problems, but rather how researchers (mis)use and (mis)interpret this tool [e.g., @greenland19; @lakens21]. As a result, many researchers present whole collections of, in their view, common *p* value misinterpretations [see, e.g., @goodman08; @greenland_etal16].

In my master thesis, I will zoom in on one specific misinterpretation: concluding *no effect* based on a statistically nonsignificant finding. Many studies have previously shown that this misinterpretation remains highly prevalent across time and sub-domains of psychology [@aczel_etal18; @hoekstra_etal06; @murphy_etal25]. In fact, in a recently published article investigating articles published in 2009, 2015, and 2021 across ten different psychology journals, we estimated the prevalence of this misinterpretation in articles’ discussion sections to lie between 76.17% and 84.90% [@murphy_etal25]. This study highlights that the situation seems not to have greatly improved despite many researchers exploring new analysis techniques [e.g., @lakens_etal18] and continuous calls to reflect on interpretations of nonsignificant results [e.g., @mcshane_etal19].

## 1.2 Possible Solutions

One frequently suggested solution is to improve researchers’ statistical literacy through enhanced education, such as better statistics teaching at the undergraduate and graduate levels [e.g., @lakens21]. However, as noted earlier, the prevalence of the misinterpretation I focus on does not seem to have substantially decreased, suggesting that calls for better education alone have not resolved the problem [@murphy_etal25]. Relatedly, researchers have also advocated for the use of interval hypotheses tests, like equivalence testing or minimum-effect tests [or the combination: three-sided testing\; @isager_fitzgerald25]. These methods allow researchers to test whether an effect is practically relevant and larger than a predefined smallest effect size of interest [SESOI\; @lakens_etal18]. In many contexts, such approaches might be more closely aligned with the substantive questions researchers aim to answer, namely whether an effect is meaningful in practice.

These strategies also align with the argument made by @lakens21 that *p* value misinterpretations represent a human factors problem, requiring practical and easy-to-implement solutions. In other contexts we encounter systems like this frequently, be it automatic braking systems in cards, word processors that flag spelling and grammar mistakes, or email clients that filter out malware and phishing attempts. Analogously, automated checks for statistical misinterpretations offer a highly promising route. This perspective emphasizes that many statistical errors arise not from bad intentions or ignorance, but from cognitive limitations and suboptimal workflows.

In the context of research, similar automated solutions are already gaining traction. For instance, the reference manager Zotero flags references to retracted papers [@stillman19]. Statcheck [@nuijten_epskamp24] automatically detects inconsistencies between reported test statistics and *p* values. Other tools, like GRIM, GRIMMER, and SPRITE, identify impossible values in reported summary statistics [@heathers_etal18]. And lastly, Regcheck [@cummin_hussey25] verifies the consistency between manuscripts and their preregistration documents.

To make the process of checking manuscripts more systematic, @R-papercheck developed Papercheck, an R package, which allows users to run a battery of checks on scientific papers. These include statistical checks (e.g., identifying imprecisely reported *p* values) as well as general manuscript quality checks (e.g., verifying links to online repositories or consistency between in-text citations and reference lists). Papercheck can be used both for single articles (e.g., as writing assistance) and for batches of articles (e.g., for meta-scientific studies). Because this framework is actively maintained and continues to evolve, the approach presented in this study was designed to fit within the Papercheck infrastructure.

# 2 Methods

## 2.1 Statement Detection, Classification and Correction

Before describing the data used in this study, it is important to understand the three steps of the proposed framework. Statements from scientific articles needed to be reliably detected, classified, and finally corrected. For each step, I applied specific methods that were best suited to achieve the respective goal.

To detect statements I used rule-based regular expressions (RegEx) and searched articles' results sections to detect these expressions. Effectively, RegEx searchers are advanced Ctrl+F searches, where a user can include rules like optional characters (e.g., 'significant(ly)' would catch both *significant* and *significantly*) and more complex rules (e.g., 'not.{0,20}significant' allows up to 20 characters between *not* and *significant*). Papercheck has a module that detects almost all *p* values (see Section 3.1 for examples of what it does not currently detect) based on RegEx searches. Using this module, I created a subset of all *p* values equal to or above .05. I then expanded the extracted nonsignificant *p* values to the full sentence and added +/- one sentence as context in case of extraction errors (incomplete statements).[^1]

[^1]: In a final Papercheck module, users will be able to set the alpha level they used themselves, thus allowing other levels than the conventional 5%.

In the next step, these statements (labeled as correct or incorrect by me; see Section 2.2) were used to train three BERT-based models. BERT (Bidirectional Encoder Representations from Transformers) is a general-purpose language model pre-trained on the BookCorpus and English Wikipedia, making it suitable for a wide range of tasks – but not specifically optimized for scientific or technical language [@devlin_etal19]. Since its introduction, many researchers have developed domain-specific variants of BERT to enhance its performance on specialized tasks. To test whether such domain adaptation improves performance in this study's classification task, I trained two models in addition to standard BERT: SciBERT was trained on a large corpus of scientific articles from Semantic Scholar, particularly in the biomedical and computer science domains [@beltagy_etal19]. PubMedBERT is an even more specific pretrained language model, having been trained exclusively on biomedical abstracts and full-text articles from the PubMed database [@gu_etal22]. These models were trained and evaluated on their ability to distinguish between correct and incorrect interpretations of nonsignificant results in scientific writing. The models’ hyperparameters (e.g., learning rate, batch size) were informed by established defaults in the field (see https://huggingface.co/docs/transformers/en/main_classes/trainer) and relevant tutorials [@more25; @talebi24], with further refinements to improve models' prediction performance.

Lastly, statements that were classified by the best performing BERT model, would, in the application of this framework, be sent to an LLM to be corrected. However, for the purposes of this validation study, I sent statements which I coded as correct or incorrect to the LLM (more on this in Section 2.2).

## 2.2 Validation Process and Performance Metrics

```{r Papercheck sample library descriptives}
#| include: false
#### Descriptive of the Sample Articles
sample_data <- papercheck::psychsci

submission_years <- c()

# Extract all submission strings
for (article in sample_data) {
  submission <- article[["info"]][["submission"]][[1]]
  submission_years <- c(submission_years, submission)
}

submission_df <- data.frame(submission_years, stringsAsFactors = FALSE)

# Extract the date following "Revision accepted"
submission_df$revision_accepted_date <- sub(
  ".*(?:Revision accepted|Accepted)[ ]*([0-9]{1,2}/[0-9]{1,2}/[0-9]{2}).*",
  "\\1",
  submission_df$submission_years
)

# Clean and normalize dates (pad month/day if needed)
split_dates <- strsplit(submission_df$revision_accepted_date, "/")

normalized_dates <- sapply(split_dates, function(x) {
  if (length(x) == 3) {
    month <- sprintf("%02d", as.numeric(x[1]))
    day   <- sprintf("%02d", as.numeric(x[2]))
    year  <- x[3]
    paste0(month, "/", day, "/", year)
  } else {
    NA
  }
})

# Convert to Date and extract year
submission_df$date_parsed <- as.Date(normalized_dates, format = "%m/%d/%y")
submission_df$publication_year <- format(submission_df$date_parsed, "%Y")

# Convert publication_year to numeric, in case it's still a character
submission_df$publication_year <- as.numeric(submission_df$publication_year)

# Get full descriptive stats
descriptives_article_year <- describe(submission_df$publication_year)

# Extract and round common statistics
median_article_year  <- round(descriptives_article_year$median, 0)
min_article_year     <- round(descriptives_article_year$min, 0)
max_article_year     <- round(descriptives_article_year$max, 0)

# Get Q1 and Q3 using base R
quartiles <- quantile(submission_df$publication_year, probs = c(0.25, 0.75), na.rm = TRUE)
q1_article_year <- round(quartiles[[1]], 0)
q3_article_year <- round(quartiles[[2]], 0)
```

To assess how well each of these three automated approaches worked, I compared each one to human ground truth and calculated appropriate measures of reliability between automated and human results.

Firstly, to ensure that the statement detection process actually caught all statements with nonsignificant *p* values in articles' results sections, I manually extracted these from 25 (10%) of the Papercheck sample library's 250 open access article from the journal Psychological Science. These articles were published between `r min_article_year` and `r max_article_year` (Median = `r median_article_year`). I then coded whether a statements I found were also extracted with the automated RegEx search.

```{r labeled data descriptives}
#| include: false
labeled_data <- read_excel("../data/training_data/labeled/labeled_data.xlsx")

labeled_results <- labeled_data[labeled_data$section == "results",]
labeled_results_99 <- labeled_results[labeled_results$label == -99,]
labeled_results_0 <- labeled_results[labeled_results$label == 0,]
labeled_results_1 <- labeled_results[labeled_results$label == 1,]

keywords_99 <- table(labeled_results_99$label_keywords)

keywords_99_flaseflag <- keywords_99[['false flag']]+keywords_99[['figure note']]
keywords_99_incomplete <- keywords_99[['incomplete']]+keywords_99[['more context needed']]
keywords_99_marginally <- keywords_99[['marginally significant']]+keywords_99[['incorrect other; marginally significant']]
keywords_99_modelfit <- keywords_99[['incomplete']]+keywords_99[['model fit; incomplete']]
```

For the training of the BERT models and to assess their final performance, I labeled all automatically extracted statements that were detected from an article's results section from Papercheck's sample library. This resulted in `r nrow(labeled_results)` statements in total. Of these, `r nrow(labeled_results_0)` were classified as containing a correct *p* value interpretation, and `r nrow(labeled_results_1)` were classified as incorrect *p* value misinterpretations. The remaining `r nrow(labeled_results_99)` statements were classified as neither correct nor incorrect because they interpreted the nonsignificant effect as (marginally) significant (`r keywords_99_marginally`), because the statements were not complete enough to check their correctness (`r keywords_99_incomplete`), because they interpreted model fit indices and not the *p* value (`r keywords_99_modelfit`), because they were falsely flagged as containing a nonsignificant *p* value (e.g., significant *p* values or generic '*p* > .05 indicated by symbol xy' statements from table/figure notes; `r keywords_99_flaseflag` in total), or due to a combination of these or other reasons (`r nrow(labeled_results_99)-keywords_99_flaseflag-keywords_99_incomplete-keywords_99_marginally-keywords_99_modelfit`).

Lastly, I reviewed 100 statements that were sent to an LLM for correction to evaluate whether the revised statements were correct. Of these, 80 had previously been labeled incorrect and 20 correct, allowing me to examine how the LLM handled false positives from the automated classification. To communicate with the models, I used Papercheck, which relies on the Groq API (available at https://groq.com/). I tested two LLMs - 'llama-3.3-70b-versatile' (created 03-09-2023) and 'openai/gpt-oss-120b' (created 05-08-2025) - and experimented with two prompts on the full validation dataset of 100 statements. This resulted in three iterations: (1) the initial prompt with the 'llama-3.3-70b-versatile' model, (2) the same prompt with 'openai/gpt-oss-120b', and (3) a refined prompt, developed through prelumanry tests on subsets of the 100 statements/validation data, with 'openai/gpt-oss-120b'.

Both prompt versions first explained to the LLM that it would receive a statement containing at least one misinterpretation of a nonsignificant finding as the absence of an effect and then instructed it to revise only the part of the statement containing this misinterpretation, while leaving the rest unchanged. The refined prompt also included additional guidance on phrasing to avoid, based on common errors that still appeared in the revisions from the initial prompt. Finally, the first prompt instructed the LLM to respond with “NO CORRECTION POSSIBLE” if it found no interpretation of a nonsignificant finding, but this instruction was removed in the refined version because the LLM overused this option, which blurred the line between statement detection, classification and correction . Both prompts are available on GitHub (see Section 2.3).

In addition to these validity checks, I also calculated performance metrics specific to the trained classifiers. For training purposes, the labeled data was split into three parts: a test set (20%) used for the final evaluation of the best model, a training set (72%, or 90% of the remaining 80%) that the model uses to learn underlying patterns and adjust its parameters, and a validation set (8%, or 10% of the 80%) used to calculate evaluation metrics after each epoch (i.e., one full cycle of the model processing the training data). Before the data was split into these three parts, I balanced the number of correct and incorrect *p* values (originally there were more correct than incorrect interpretations) to ensure that the model would not overfit to this class-imbalance.

```{r BERT performance}
#| include: false
#################################
#####                       #####
#####     Standard BERT     #####
#####                       #####
#################################

#### Load data
classification_report_BERT <- read.csv("../data/model_performance/classification_report/BERT.csv")
training_history_BERT <- read.csv("../data/model_performance/model_training_history/BERT.csv")
test_predictions_BERT <-read.csv("../data/model_performance/test_predictions/BERT.csv")

#### Data preparation
  # Training History

# Fill NAs in selected columns using the value from the previous row
training_history_BERT <- training_history_BERT |>
  fill(loss, grad_norm, learning_rate, .direction = "down")

# Remove odd-numbered rows
training_history_BERT <- training_history_BERT[seq(2, nrow(training_history_BERT), by = 2), ]

#### Preparing inline citation
  # Classification report
precision_0_BERT <- apa_p(classification_report_BERT$precision[1], 2)
precision_1_BERT <- apa_p(classification_report_BERT$precision[2], 2)
recall_0_BERT <- apa_p(classification_report_BERT$recall[1], 2)
recall_1_BERT <- apa_p(classification_report_BERT$recall[2], 2)
F1_0_BERT <- apa_p(classification_report_BERT$f1.score[1], 2)
F1_1_BERT <- apa_p(classification_report_BERT$f1.score[2], 2)

macro_F1_BERT <- apa_p(classification_report_BERT$f1.score[4], 2)

n_total_BERT <- round(classification_report_BERT$support[5], 0)

  # Training History
n_epochs_BERT <- max(training_history_BERT$epoch)
best_epoch_BERT <- training_history_BERT$epoch[training_history_BERT$eval_loss == min(training_history_BERT$eval_loss)]
  
  # Model Predictions
TP_BERT <- nrow(filter(test_predictions_BERT, test_predictions_BERT$human_label == 1 & test_predictions_BERT$BERT_label == 1))
TN_BERT <- nrow(filter(test_predictions_BERT, test_predictions_BERT$human_label == 0 & test_predictions_BERT$BERT_label == 0))
FP_BERT <- nrow(filter(test_predictions_BERT, test_predictions_BERT$human_label == 0 & test_predictions_BERT$BERT_label == 1))
FN_BERT <- nrow(filter(test_predictions_BERT, test_predictions_BERT$human_label == 1 & test_predictions_BERT$BERT_label == 0))

#################################
#####                       #####
#####        SciBERT        #####
#####                       #####
#################################

#### Load data
classification_report_SciBERT <- read.csv("../data/model_performance/classification_report/SciBERT.csv")
training_history_SciBERT <- read.csv("../data/model_performance/model_training_history/SciBERT.csv")
test_predictions_SciBERT <-read.csv("../data/model_performance/test_predictions/SciBERT.csv")

#### Data preparation
  # Training History

# Fill NAs in selected columns using the value from the previous row
training_history_SciBERT <- training_history_SciBERT |>
  fill(loss, grad_norm, learning_rate, .direction = "down")

# Remove odd-numbered rows
training_history_SciBERT <- training_history_SciBERT[seq(2, nrow(training_history_SciBERT), by = 2), ]

#### Preparing inline citation
  # Classification report
precision_0_SciBERT <- apa_p(classification_report_SciBERT$precision[1], 2)
precision_1_SciBERT <- apa_p(classification_report_SciBERT$precision[2], 2)
recall_0_SciBERT <- apa_p(classification_report_SciBERT$recall[1], 2)
recall_1_SciBERT <- apa_p(classification_report_SciBERT$recall[2], 2)
F1_0_SciBERT <- apa_p(classification_report_SciBERT$f1.score[1], 2)
F1_1_SciBERT <- apa_p(classification_report_SciBERT$f1.score[2], 2)

macro_F1_SciBERT <- apa_p(classification_report_SciBERT$f1.score[4], 2)

n_total_SciBERT <- round(classification_report_SciBERT$support[5], 0)

  # Training History
n_epochs_SciBERT <- max(training_history_SciBERT$epoch)
best_epoch_SciBERT <- training_history_SciBERT$epoch[training_history_SciBERT$eval_loss == min(training_history_SciBERT$eval_loss)]

  # Model Predictions
TP_SciBERT <- nrow(filter(test_predictions_SciBERT, test_predictions_SciBERT$human_label == 1 & test_predictions_SciBERT$BERT_label == 1))
TN_SciBERT <- nrow(filter(test_predictions_SciBERT, test_predictions_SciBERT$human_label == 0 & test_predictions_SciBERT$BERT_label == 0))
FP_SciBERT <- nrow(filter(test_predictions_SciBERT, test_predictions_SciBERT$human_label == 0 & test_predictions_SciBERT$BERT_label == 1))
FN_SciBERT <- nrow(filter(test_predictions_SciBERT, test_predictions_SciBERT$human_label == 1 & test_predictions_SciBERT$BERT_label == 0))

#################################
#####                       #####
#####      PubMedBERT       #####
#####                       #####
#################################

#### Load data
classification_report_PubMedBERT <- read.csv("../data/model_performance/classification_report/PubMedBERT.csv")
training_history_PubMedBERT <- read.csv("../data/model_performance/model_training_history/PubMedBERT.csv")
test_predictions_PubMedBERT <-read.csv("../data/model_performance/test_predictions/PubMedBERT.csv")

#### Data preparation
  # Training History

# Fill NAs in selected columns using the value from the previous row
training_history_PubMedBERT <- training_history_PubMedBERT |>
  fill(loss, grad_norm, learning_rate, .direction = "down")

# Remove odd-numbered rows
training_history_PubMedBERT <- training_history_PubMedBERT[seq(2, nrow(training_history_PubMedBERT), by = 2), ]

#### Preparing inline citation
  # Classification report
precision_0_PubMedBERT <- apa_p(classification_report_PubMedBERT$precision[1], 2)
precision_1_PubMedBERT <- apa_p(classification_report_PubMedBERT$precision[2], 2)
recall_0_PubMedBERT <- apa_p(classification_report_PubMedBERT$recall[1], 2)
recall_1_PubMedBERT <- apa_p(classification_report_PubMedBERT$recall[2], 2)
F1_0_PubMedBERT <- apa_p(classification_report_PubMedBERT$f1.score[1], 2)
F1_1_PubMedBERT <- apa_p(classification_report_PubMedBERT$f1.score[2], 2)

macro_F1_PubMedBERT <- apa_p(classification_report_PubMedBERT$f1.score[4], 2)

n_total_PubMedBERT <- round(classification_report_PubMedBERT$support[5], 0)

  # Training History
n_epochs_PubMedBERT <- max(training_history_PubMedBERT$epoch)
best_epoch_PubMedBERT <- training_history_PubMedBERT$epoch[training_history_PubMedBERT$eval_loss == min(training_history_PubMedBERT$eval_loss)]

  # Model Predictions
TP_PubMedBERT <- nrow(filter(test_predictions_PubMedBERT, test_predictions_PubMedBERT$human_label == 1 & test_predictions_PubMedBERT$BERT_label == 1))
TN_PubMedBERT <- nrow(filter(test_predictions_PubMedBERT, test_predictions_PubMedBERT$human_label == 0 & test_predictions_PubMedBERT$BERT_label == 0))
FP_PubMedBERT <- nrow(filter(test_predictions_PubMedBERT, test_predictions_PubMedBERT$human_label == 0 & test_predictions_PubMedBERT$BERT_label == 1))
FN_PubMedBERT <- nrow(filter(test_predictions_PubMedBERT, test_predictions_PubMedBERT$human_label == 1 & test_predictions_PubMedBERT$BERT_label == 0))
```

During BERT training I computed the training loss (sum of errors between model predictions and actual labels in the training set) and the validation loss (same for validation set). The best-performing model was selected based on the lowest validation loss to prevent the model from overfitting to the training data. The model would have been trained on a maximum of 16 epochs, but training ended early if the model did not improve, as measured by the validation loss, for two epochs. Ultimately, the longest number of training epochs was `r max(c(n_epochs_BERT, n_epochs_SciBERT, n_epochs_PubMedBERT))`. For final evaluation, I computed the fraction of correctly predicted classes among all predicted cases of a class (precision), the fraction of correctly predicted classes among all actual cases of a class (recall), and their harmonic mean (F1 score), separately for each class. To summarize overall performance across the two classes, I calculated the unweighted average of the two F1 scores (macro-F1 score).

## 2.3 Software

All scripts for this thesis project were written in R [Version 4.5.0\; @rcoreteam25] or Python [Version 3.12.10\; @python].

In R, I used *papercheck* [Version 0.0.0.9049\; @R-papercheck] for accessing the 250 open access articles, preprocess them and for communication with the LLM, *readxl* [Version 1.4.5\; @R-readxl] to access Excel files in R, *psych* [Version 2.5.6\; @R-psych] for calculating descriptive statistics, *tidyverse* [Version 2.0.0\; @R-tidyverse] for data preprocessing and visualization, and *flextable* [Version 0.9.9\; @R-flextable], *magick* [Version 2.8.7\; @R-magick], *papaja* [Version 0.1.3\; @R-papaja] and *showtext* [Version 0.9-7\; @R-showtext] to create APA-formatted tables and figures.

All scripts and data to reproduce and use the trained BERT models (Python), analyse the results and validity checks (R and Python), recreate this manuscript (Quarto Markdown in R Studio with the apaquarto extension available at: https://wjschne.github.io/apaquarto/), as well as the list of Python libraries used to train the BERT models are available in this GitHub repository, together with instructions on how to set it up: LINK.

Due to the project's iterative nature and since no inferential statistical tests were performed, this study was not preregistered. The original project proposal can also be found on the GitHub repository.

# 3 Results

## 3.1 Detection Accuracy

```{r detection_check data}
#| include: false

detection_check <- read_excel("../data/detection_check/detection_checked.xlsx")

agreement_table <- table(detection_check$agreement)
agreement_1 <- agreement_table[['1']]
agreement_2 <- agreement_table[['2']]
agreement_3 <- agreement_table[['3']]
agreement_3_FP <- sum(detection_check$agreement == 3 & detection_check$agree_3_FP_FN == "FP", na.rm = TRUE)
agreement_3_FN <- sum(detection_check$agreement == 3 & detection_check$agree_3_FP_FN == "FN", na.rm = TRUE)

keywords_table <- table(detection_check$disagreement_keyword)
keywords_figure <- keywords_table[['figure note']]
keywords_footnote <- keywords_table[['footnote']]
keywords_formatting <- keywords_table[['formatting']]
keywords_character <- keywords_table[['not sure; unusual character']]
keywords_pagebreak <- keywords_table[['page break']]
keywords_tablecontent <- keywords_table[['table content']]
keywords_ns <- keywords_table[['n.s.']]
keywords_ps <- keywords_table[['p_s']]
```

Manually going through 25 articles from Papercheck's sample library I detected `r agreement_1+agreement_2+agreement_3_FN` statements with a nonsignificant *p* value. The automated RegEx search caught `r agreement_1` (`r round(100*(agreement_1/(agreement_1+agreement_2+agreement_3_FN)))` %) of these completely, and `r agreement_3_FN` partially (incompletely) due to extraction errors (e.g., because of pdf formatting like page breaks, figures or footnotes). It also 'found' `r agreement_3_FP` false positives in the sense that it extracted 'statements' from tables or figure notes or ones that were misclassified as coming from a results section. Note, however, that the large majority of the total `r agreement_2+agreement_3_FN` missed statements were due to specific ways of writing (or not writing) the *p* value: `r keywords_ns` were missed because the authors wrote 'n.s.' instead of the nonsignificant *p* value, and `r keywords_ps` were missed because the *p* value was written as '$p_s$'. Without these two mistakes the overall agreement of automated and manual approach would have been `r round(100*(agreement_1/(agreement_1+agreement_2+agreement_3_FN-keywords_ns-keywords_ps)))` %.

Most of the other misses were due to pdf formatting issues like figures, tables, footnotes and page breaks or unusual characters inside the statement that interfered with the statement extraction (`r keywords_figure+keywords_footnote+keywords_formatting+keywords_character+keywords_pagebreak+keywords_tablecontent` in total).[^2]

[^2]: I could not find one statement that was extracted automatically in the artilce's pdf. My current theory is that this was an artifact from when the pdf was compiled and might be from a different article even, once again highlighting how impractical the pdf format is in times of increasing automation.

## 3.2 Classification Performance

@fig-loss-curves shows the training and validation loss curves for the three BERT models across their training epochs. The standard BERT model was trained for a total of `r n_epochs_BERT` epochs before early stopping was triggered due to a lack of improvement in validation loss for two consecutive epochs. The model from epoch `r best_epoch_BERT` was therefore selected as the best-performing one. Similarly, SciBERT and PubMedBERT reached the lowest validation loss after epochs `r best_epoch_SciBERT` and `r best_epoch_PubMedBERT`, respectively. As shown in the figure, the training loss consistently decreased over time for all three models, as expected given that models were optimized to fit the training data. In contrast, the validation loss plateaued in all models before increasing again, indicating that further improvements in fitting the training data no longer translated into better performance on unseen data and may even signal the onset of overfitting.

```{r Combine model loss curves into one figure}
#| include: false
library(magick)

# Read all three PDFs (first page of each)
loss_curve_bert     <- image_read_pdf("../data/model_performance/loss_curve/BERT.pdf")
loss_curve_scibert  <- image_read_pdf("../data/model_performance/loss_curve/SciBERT.pdf")
loss_curve_pubmed   <- image_read_pdf("../data/model_performance/loss_curve/PubMedBERT.pdf")

# Combine them horizontally
combined <- image_append(c(loss_curve_bert, loss_curve_scibert, loss_curve_pubmed))

# Save to a new PDF
image_write(combined, path = "../data/model_performance/loss_curve/loss_combined.png", format = "png")
```

![Training and Validation Loss Curve](../data/model_performance/loss_curve/loss_combined.png){#fig-loss-curves width="6in" fig-alt="Curves of the training and validation loss of the three trained BERT models. The best models for regular BERT, SciBERT and PubMedBERT were chosen after epoch `r best_epoch_BERT`, `r best_epoch_SciBERT`, and `r best_epoch_PubMedBERT`, respectively, based on the minumum validation loss." apa-note="Curves of the training and validation loss of the three trained BERT models. The best models for regular BERT, SciBERT and PubMedBERT were chosen after epoch `r best_epoch_BERT`, `r best_epoch_SciBERT`, and `r best_epoch_PubMedBERT`, respectively, based on the minumum validation loss."}

Moving to model performance, @fig-confusion shows the number of correctly and incorrectly classified statements for each model. In general, all three models performed well. Overall, the standard BERT model showed the fewest misclassifications with `r FP_BERT` false positives and `r FN_BERT` false negatives, whereas SciBERT and PubMedBERT showed `r FP_SciBERT+FN_SciBERT` and `r FP_PubMedBERT+FN_PubMedBERT` false classifications in total, respectively. Zooming out, these results are also visible in @tbl-model-performance, which summarizes the performance metrics. As reflected in the macro F1 score, again, BERT achieved the best overall performance in classifying correct and incorrect statements with a macro F1 score of `r macro_F1_BERT`. SciBERT and PubMedBERT lagged slightly behind, with macro F1 scores of `r macro_F1_SciBERT` and `r macro_F1_PubMedBERT`, respectively. All three models better predicted correct statements than incorrect ones, reflected by the F1 score of the 'correct' class, with standard BERT scoring best ('correct' F1 score of `r F1_0_BERT`). Similarly, in all models, recall was higher than precision in the 'correct' class, whereas the opposite pattern was visible in the 'incorrect' class, suggesting that the models tend to err on the side of overidentifying statements as correct rather than incorrect. In fact, the standard BERT model was just slightly better at reducing false negatives (at the cost of more false positives) in this test set (Precision in the 'incorrect' class: `r precision_1_BERT` vs. SciBERT's `r precision_1_SciBERT`).

```{r Combine model confusion matrices into one figure}
#| include: false
library(magick)

# Read all three PDFs (first page of each)
confusion_matrix_bert     <- image_read_pdf("../data/model_performance/confusion_matrix/BERT.pdf")
confusion_matrix_scibert  <- image_read_pdf("../data/model_performance/confusion_matrix/SciBERT.pdf")
confusion_matrix_pubmed   <- image_read_pdf("../data/model_performance/confusion_matrix/PubMedBERT.pdf")

# Combine them horizontally
combined <- image_append(c(confusion_matrix_bert, confusion_matrix_scibert, confusion_matrix_pubmed))

# Save to a new PDF
image_write(combined, path = "../data/model_performance/confusion_matrix/confusion_matrix_combined.png", format = "png")
```

![Confusion Matrix](../data/model_performance/confusion_matrix/confusion_matrix_combined.png){#fig-confusion width="6.5in" fig-alt="Confusion matrices of the three trained BERT models." apa-note="Confusion matrices of the three trained BERT models."}

```{r}
#| label: tbl-model-performance
#| message: false
#| warning: false
#| apa-note: Table of precision, recall and F1 score per model and class.
#| ft-align: left
#| tbl-cap: Model Performance

# Create the data
tbl <- tibble(
  ' '            = c("Correct Class", "Incorrect Class", "Macro F1 score"),
  'BERT 1'       = c(precision_0_BERT, precision_1_BERT, ""),
  'BERT 2'       = c(recall_0_BERT, recall_1_BERT, ""),
  'BERT 3'       = c(F1_0_BERT, F1_1_BERT, macro_F1_BERT),
  'SciBERT 1'    = c(precision_0_SciBERT, precision_1_SciBERT, ""),
  'SciBERT 2'    = c(recall_0_SciBERT, recall_1_SciBERT, ""),
  'SciBERT 3'    = c(F1_0_SciBERT, F1_1_SciBERT, macro_F1_SciBERT),
  'PubMedBERT 1' = c(precision_0_PubMedBERT, precision_1_PubMedBERT, ""),
  'PubMedBERT 2' = c(recall_0_PubMedBERT, recall_1_PubMedBERT, ""),
  'PubMedBERT 3' = c(F1_0_PubMedBERT, F1_1_PubMedBERT, macro_F1_PubMedBERT)
)

# Create the flextable with grouped headers
tbl %>%
  flextable() %>%
  set_header_labels(
    `BERT 1` = "Precision", `BERT 2` = "Recall", `BERT 3` = "F1 score",
    `SciBERT 1` = "Precision", `SciBERT 2` = "Recall", `SciBERT 3` = "F1 score",
    `PubMedBERT 1` = "Precision", `PubMedBERT 2` = "Recall", `PubMedBERT 3` = "F1 score"
  ) %>%
  add_header_row(
  values = c(" ", "BERT", "SciBERT", "PubMedBERT"),
  colwidths = c(1, 3, 3, 3)
  ) %>%
  flextable::fit_to_width(max_width = 6.5) %>%
  fontsize(size = 11, part = "all") %>%
  flextable::theme_apa()
```

@tbl-false-classification shows statements misclassified by all three models to illustrate common sources of difficulty. Potential causes of these misclassifications will be explored in the discussion.

```{r}
#| label: tbl-false-classification
#| message: false
#| warning: false
#| apa-note: Examples for incorrect classifications of the trained SciBERT model on the test data.
#| ft-align: left
#| tbl-cap: Incorrect SciBERT Classifications

# Create the data
tibble(
  'Model Prediction'  = c("False Negative", "False Negative" ,"False Negative", "False Positive"),
  'Statement'         = c(test_predictions_BERT$statement[6], test_predictions_BERT$statement[16], # 6 is a false negative in all models, 
                          test_predictions_BERT$statement[83], test_predictions_BERT$statement[4])) %>%
  flextable() %>%
  width(j = 1, width = 1) %>%   # Set width of column 1 (Error)
  width(j = 2, width = 5) %>%   # Set width of column 1 (Statement)
  #fontsize(size = 11, part = "all") %>%
  flextable::theme_apa()
```

## 3.3 Correction Evaluation

```{r}
#| include: false
#####################
####             ####
#### Version 1.0 ####
####             ####
#####################

llm_corrections_1.0 <- read_excel("../data/llm_correction_check/llm_correction_checked_1.0.xlsx")

n_correct_1.0 <- sum(llm_corrections_1.0$corrected_correct == 0)
n_incorrect_1.0 <- sum(llm_corrections_1.0$corrected_correct == 1)

n_0_correct_1.0 <- sum(llm_corrections_1.0$label == 0 & llm_corrections_1.0$corrected_correct == 0)
n_0_incorrect_1.0 <- sum(llm_corrections_1.0$label == 0 & llm_corrections_1.0$corrected_correct == 1)

n_1_correct_1.0 <- sum(llm_corrections_1.0$label == 1 & llm_corrections_1.0$corrected_correct == 0)
n_1_incorrect_1.0  <- sum(llm_corrections_1.0$label == 1 & llm_corrections_1.0$corrected_correct == 1)

#####################
####             ####
#### Version 1.1 ####
####             ####
#####################

llm_corrections_1.1 <- read_excel("../data/llm_correction_check/llm_correction_checked_1.1.xlsx")

n_correct_1.1 <- sum(llm_corrections_1.1$corrected_correct == 0)
n_incorrect_1.1 <- sum(llm_corrections_1.1$corrected_correct == 1)

n_0_correct_1.1 <- sum(llm_corrections_1.1$label == 0 & llm_corrections_1.1$corrected_correct == 0)
n_0_incorrect_1.1 <- sum(llm_corrections_1.1$label == 0 & llm_corrections_1.1$corrected_correct == 1)

n_1_correct_1.1 <- sum(llm_corrections_1.1$label == 1 & llm_corrections_1.1$corrected_correct == 0)
n_1_incorrect_1.1  <- sum(llm_corrections_1.1$label == 1 & llm_corrections_1.1$corrected_correct == 1)

n_0_not_corrected_1.1 <- sum(llm_corrections_1.1$label == 0 & llm_corrections_1.1$corrected == "NO CORRECTION POSSIBLE")
n_1_not_corrected_1.1 <- sum(llm_corrections_1.1$label == 1 & llm_corrections_1.1$corrected == "NO CORRECTION POSSIBLE")

#####################
####             ####
#### Version 2.0 ####
####             ####
#####################

llm_corrections_2.0 <- read_excel("../data/llm_correction_check/llm_correction_checked_2.0.xlsx")

n_correct_2.0 <- sum(llm_corrections_2.0$corrected_correct == 0)
n_incorrect_2.0 <- sum(llm_corrections_2.0$corrected_correct == 1)

n_0_correct_2.0 <- sum(llm_corrections_2.0$label == 0 & llm_corrections_2.0$corrected_correct == 0)
n_0_incorrect_2.0 <- sum(llm_corrections_2.0$label == 0 & llm_corrections_2.0$corrected_correct == 1)

n_1_correct_2.0 <- sum(llm_corrections_2.0$label == 1 & llm_corrections_2.0$corrected_correct == 0)
n_1_incorrect_2.0  <- sum(llm_corrections_2.0$label == 1 & llm_corrections_2.0$corrected_correct == 1)
```

Of the 100 statements that the LLM was instructed to correct `r n_correct_1.0` were judged as correct. Notably, `r n_0_incorrect_1.0` of the 20 already correct statements were turned incorrect by the LLM, and `r n_1_incorrect_1.0` of the 80 incorrect statements remained incorrect. Using the newer 'openai/gpt-oss-120b' model, `r n_correct_1.1` were correct. However, the model overused the 'NO CORRECTION POSSIBLE' option (originally intended to prevent detection errors) which resulted in `r n_0_not_corrected_1.1` correct and `r n_1_not_corrected_1.1` incorrect statements being left unrevised. In the final iteration, with the newer model and revised prompt, `r n_correct_2.0` of the 100 statements were correct. Of the remaining `r n_incorrect_2.0` statements, XXX were originally incorrect statement that remained incorrect, whereas the other YYY did not misinterpret nonsignificance but contained other problems: in one case, the LLM removed all *p* values and interpreted the effects as significant, and in another, it altered the statement's meaning substantially. Examples of both poor and strong LLM-revisions from this final iteration are shown in @tbl-LLM-corrections-1 and @tbl-LLM-corrections-2, respectively.

**Above paragraph needs to be updated after discussion on how to code some of them with Daniel! Also need to update below tables after that**

```{r}
#| label: tbl-LLM-corrections-1
#| message: false
#| warning: false
#| apa-note: Table of original and LLM-revised statements that were classified as incorrect. In the examples '0' refers to correct and '1' to incorrect.
#| ft-align: left
#| tbl-cap: Examples of Incorrect LLM-Revisions

# Your selected example indices
examples <- c(60, 80)

# Fix 'ÃŽÂ²' in statements --> should be beta
llm_corrections_2.0$statement <- gsub("ÃŽÂ²|ÃŽÂ²", "β", llm_corrections_2.0$statement)
llm_corrections_2.0$corrected <- gsub("ÃŽÂ²|ÃŽÂ²", "β", llm_corrections_2.0$corrected)

# Create long format table
df_long <- map_dfr(examples, function(i) {
  example_label <- paste0("Example ", llm_corrections_2.0$label[i], " to ", llm_corrections_2.0$corrected_correct[i])
  
  tibble(
    Example = rep(example_label, 2),
    `Statement Type` = c("Original", "LLM-revised"),
    Statement = c(llm_corrections_2.0$statement[i], llm_corrections_2.0$corrected[i]),
    Label = as.character(c(llm_corrections_2.0$label[i], llm_corrections_2.0$corrected_correct[i]))
  )
})

# Create the table with merged 'Example' cells
df_long %>%
  flextable() %>%
  flextable::theme_apa() %>%
  merge_v(j = ~Example) %>%       # Merge vertical cells in the 'Example' column
  valign(j = ~Example, val = "center") %>%  # Align merged cells at the top
  line_spacing(j = c(1:3), space = 1.2) %>%  # Set line spacing in Statement column
  # fontsize(size = 9, part = "all") %>%
  width(j = 1, width = 0.7) %>%   # Set width of column 1 (Example)
  width(j = 2, width = 0.7) %>%   # Column 2 (Statement Type)
  width(j = 3, width = 4) %>%   # Column 3 (Statement)
  width(j = 4, width = 0.7) #%>%   # Column 4 (Label)
  # autofit() %>%
```

```{r}
#| label: tbl-LLM-corrections-2
#| message: false
#| warning: false
#| apa-note: Table of original and LLM-revised statements that were classified as correct. In the examples '0' refers to correct and '1' to incorrect.
#| ft-align: left
#| tbl-cap: Examples of Correct LLM-Revisions

# Your selected example indices
examples <- c(10, 79)

# Fix 'ÃŽÂ²' in statements --> should be beta
llm_corrections_2.0$statement <- gsub("ÃŽÂ²|ÃŽÂ²", "β", llm_corrections_2.0$statement)
llm_corrections_2.0$corrected <- gsub("ÃŽÂ²|ÃŽÂ²", "β", llm_corrections_2.0$corrected)

# Create long format table
df_long <- map_dfr(examples, function(i) {
  example_label <- paste0("Example ", llm_corrections_2.0$label[i], " to ", llm_corrections_2.0$corrected_correct[i])
  
  tibble(
    Example = rep(example_label, 2),
    `Statement Type` = c("Original", "LLM-revised"),
    Statement = c(llm_corrections_2.0$statement[i], llm_corrections_2.0$corrected[i]),
    Label = as.character(c(llm_corrections_2.0$label[i], llm_corrections_2.0$corrected_correct[i]))
  )
})

# Create the table with merged 'Example' cells
df_long %>%
  flextable() %>%
  flextable::theme_apa() %>%
  merge_v(j = ~Example) %>%       # Merge vertical cells in the 'Example' column
  valign(j = ~Example, val = "center") %>%  # Align merged cells at the top
  line_spacing(j = c(1:3), space = 1.2) %>%  # Set line spacing in Statement column
  # fontsize(size = 9, part = "all") %>%
  width(j = 1, width = 0.7) %>%   # Set width of column 1 (Example)
  width(j = 2, width = 0.7) %>%   # Column 2 (Statement Type)
  width(j = 3, width = 4) %>%   # Column 3 (Statement)
  width(j = 4, width = 0.7) #%>%   # Column 4 (Label)
  # autofit() %>%
```

# 4 Discussion

**Things I still need do add:**

- GO back to human factors component, maybe int he part about LLM-corrections?
- I don't yet discuss why BERT might have show the better performance compared to the other two models
- I don't really discuss the reasons for misclassifications contrary to that I say in the results section

## 4.1 Summary of Key Results

In this study, I developed and evaluated a three-step pipeline for automatically correcting misinterpretations of nonsignificant results as evidence for the absence of an effect. The approach combines rule-based RegEx searches for detecting candidate statements, fine-tuned BERT models for classification, and LLMs to generate corrected phrasings. While each step leaves room for improvement, the overall pipeline performed well and shows promise for broader applications beyond *p* value misinterpretations.

Crucially, the framework works effectively because each step is tailored to a specific subtask in the correction process. The RegEx-based detection offers a fast, systematic, and transparent way of reducing the volume of text needing NLP-based analysis. The BERT models, having been trained on human-annotated data, provide a reliable and powerful solution for learning subtle language patterns. Finally, the optional LLM correction might enhance the user experience further by offering useful rewording suggestions tailored to the context of the specific statement. This layered, hybrid-NLP structure makes the approach both flexible and easily scalable.

The RegEx-based statement detection phase demonstrated that simple, rule-based searches can effectively flag a large proportion of candidate interpretations. Although formatting issues in pdfs made the correct extraction of these misinterpretations impossible in some cases, the vast majority of statements were automatically detected. In addition, the study revealed straightforward issues in the current approach (e.g., as "p = n.s." or with subscripted '$p_s$' ) that can be fixed with minimal adjustments, further enhancing detection accuracy.

At the same time, the classification results are particularly promising given the relatively small size of the manually labeled dataset (< 1,000 examples, split into training, validation, and test sets). The strong performance likely reflects a certain regularity in how nonsignificance is (mis)interpreted in academic writing - commonly through the use of either 'significant' or 'no effect' (e.g., 'there was no effect', but also 'groups did not differ') terminology. While the training dataset was limited to statements extracted from Psychological Science articles (using the existing Papercheck sample library), the results provide a solid baseline for expansion using more diverse sources and research domains.

The final step, generating LLM-revised corrections of the original statements, showed clear promise, indicating that LLMs can, in principle, be used to suggest improvements to authors’ interpretations. Based on the current results, further prompt engineering may help address persisting issues (e.g., correct statements being turned into incorrect ones). It remains to be tested empirically, however, whether such optional feedback is actually needed for authors to revise their interpretations, or whether simply flagging statements as incorrect might already be sufficient.

## 4.2 Limitations and Challenges

Despite the encouraging results, several limitations must be acknowledged. At the most technical level, the pipeline components were evaluated independently rather than as a fully integrated system. While each step (detection, classification, correction) showed strong performance on its own, cascading errors in a full pipeline will likely reduce overall accuracy. Still, any flagged misinterpretation should alert authors that their interpretation may require reconsideration.

A further limitation concerns the manual annotation of training data, which inevitably introduces subjectivity. I made efforts to standardize labels - often consulting a statistics expert (my supervisor) on difficult or borderline cases - but ultimately, the classifications reflect my interpretation of what constitutes a misinterpretation. Ideally, multiple annotators and inter-rater agreement metrics would strengthen the reliability and generalizability of the dataset. The fact that the fine-tuned BERT models generalized well to unseen data suggests that the labeling was systematic enough for the models to learn, but some readers may view certain decisions differently. I therefore welcome reruns of the models with alternative annotations.

A more practical challenge involves managing the tradeoff between false positives and false negatives. The current models aim to balance both for optimal macro performance (as seen in the results, this was not perfectly possible). However, in practice, different use cases may prioritize one over the other. For example, an individual researcher using the system to improve their writing may prefer fewer false negatives (i.e., catching as many problematic statements as possible), even at the cost of some false positives. Conversely, a meta-scientist analyzing prevalence trends of this misinterpretation may prioritize precision to avoid overestimating misinterpretations. This issue can be mitigated by allowing users to adjust the model's decision threshold for predicting one label or the other. A future Papercheck module based on this work could incorporate such functionality to fit users’ specific goals.

Another limitation involves the narrow context in which statements are classified (a single sentence containing a nonsignificant *p* value). This limited scope means the model cannot account for broader contextual factors, such as whether authors conducted equivalence testing, reported Bayesian results, or provided qualifying language elsewhere in the manuscript. As noted earlier, however, this pipeline’s feedback should prompt authors to reconsider their interpretations beyond the single detected statement.

Despite the narrow context, results showed that the classifiers had problems with XY...

Beyond technical and annotation-related issues, there is the broader question of whether full automation of the correction process is even desirable. While this study demonstrates that LLMs can generate useful rephrasings, one might argue that automated flagging alone should suffice, leaving authors themselves responsible for revising their interpretations. Unlike a spell-checker, which corrects mechanical errors, misinterpretations of statistical results often reflect deeper conceptual issues. An overreliance on automated corrections risks shifting responsibility away from researchers and could even encourage passivity in scientific writing. In this sense, the pipeline should be seen primarily as a tool to prompt reflection, not as a substitute for critical thinking.

Finally, one might wonder whether rapid advances in AI could soon render a pipeline like this obsolete. While the future is uncertain, prior research suggests that simple classifiers trained for specific tasks can still outperform general zero-shot applications of modern LLMs [@bucher_martini24]. This may of course change as LLMs improve, but the step-wise approach used in this study offers distinct advantages: it avoids the inefficiency of sending entire papers to an LLM and simply taking the LLM’s output at face value, and instead applies the simplest sufficient method at each stage. This design makes the process both more transparent and more resource-efficient.

## 4.3 Practical Use and Future Directions

The pipeline described in this study will be integrated into a new Papercheck module for identifying potential misinterpretations of nonsignificant results. Some clear improvements have been noticed through this study: Firstly, the current RegEx searches of Papercheck's "all_p_values" module might not be optimized to detect all different ways in which a p values can be written, e.g., the previously mentioned $p_s$ is often used to refer to the smallest p value in some collection of tests. This is an example of usually irrelevant RegEx's that I will add to improve this automatic detection of candidate statements. Additionally, the dataset used to train the BERT models will also be expanded and re-checked by independent coders to ensure that the aspects the models do pick up are generalizable. Lastly, mistakes from the correction validity check of statements' LLM-revised corrections will be closely analyzed to inform further prompt engineering to reduce any mistakes.

Importantly, the pipeline’s step-wise structure makes it easy to adapt to other classification or correction tasks. For instance, users could train custom classifiers to detect different issues in reporting practices [see @vanabkoude25 for an application to problematic causal language]. In practice, this would involve specifying RegEx patterns that capture the target aspects, training classifiers to label them as correct or incorrect, and, if desired, creating a prompt to generate corrections. Depending on the issue of interest, such classifiers could also be trained on existing hand-labeled datasets from meta-scientific studies where researchers coded specific practices or mistakes [e.g., @aczel_etal18].

Going forward, an important next step will be conducting qualitative user studies to explore how authors would prefer such a tool to be designed and implemented. A central question will be the role of the optional correction feature - whether authors find value in receiving suggested corrections or whether simple flagging is sufficient. These studies could also reveal where customization is most useful (e.g., varying levels of strictness, setting a personal alpha level instead of the conventional 5%, etc.). In addition, experimental evaluations would help assess whether the tool reduces the prevalence of misinterpretations and increases authors’ awareness of them.

# 5 Conclusion

This study demonstrates that a hybrid rule-based and NLP-driven pipeline can effectively detect, classify, and correct a common statistical misinterpretation in scientific writing: interpreting nonsignificant results as evidence for the absence of an effect. Each step - statement detection, classification, and correction - performed well independently. The next step is to evaluate the pipeline as a fully automated system in real-world use cases. With further refinement, this framework has the potential to enhance both automated manuscript checks and large-scale meta-scientific analyses at scale.

# Achknowledgement

I want to thank Dr. Daniël Lakens for his constant support throughout this thesis and for a wonderful research stay that allowed me to work on it in person. I thank my partner for her unwavering support, for encouraging me when I felt discouraged, and for very insightful discussions on whether and how we (should and) should not use AI. I also thank Prof. Dr. Maike Luhmann for allowing me to pursue a meta-scientific project that is so close to my heart, even though it falls somewhat outside her area of expertise. Finally, I thank Christian Sodano for helpful discussions on machine learning and BERT models, which helped to ensure that my model training did not turn into 'algorithmic *p*-hacking'.
 

# References

<!-- References will auto-populate in the refs div below -->

::: {#refs}
:::

