---
title: "From Detection to Correction: A Hybrid NLP Approach to Misinterpretations of Nonsignificant *p* Values"
shorttitle: "Hybrid NLP for Correcting *p* Value Misinterpretations"
author:
  - name: Raphael Merz
    corresponding: true
    orcid: 0000-0002-9474-3379
    email: raphael.merz@rub.de
    affiliations:
      - name: Ruhr University Bochum
        department: Department of Psychology
        city: Bochum
        country: Germany
author-note:
  status-changes: 
    affiliation-change: null
    deceased: null
  disclosures:
    study-registration: null
    data-sharing: null
    related-report: null
    conflict-of-interest: null
    financial-support: null
    gratitude: null
    authorship-agreements: null
abstract: "Misinterpretations of *p* values remain a highly prevalent issue in scientific reporting, despite decades of educational efforts and reform initiatives. Among the most frequent and consequential misinterpretations is the conclusion that a statistically nonsignificant result (e.g., *p* > .05) implies the absence of an effect – a claim not supported by the logic of null hypothesis significance testing (NHST). This project draws on a human factors perspective, arguing that automation can offer practical, scalable solutions to persistent statistical errors – comparable to how word processors flag potential spelling and grammar mistakes. This master’s thesis project proposes the development of an automated tool to detect, classify, and correct such misinterpretations using a combination of rule-based searches, large language models (LLMs), and machine learning classifiers. Building on the existing papercheck framework – an R package created to make automated checks of academic manuscripts easier and more systematic – the project aims to identify statements interpreting nonsignificant results, determine whether these interpretations are correct, and suggest improved phrasing if they are not. Initial detection will rely on rule-based text searches to locate candidate sentences, which will then be filtered and contextualized using LLMs. Classification of interpretations as correct or incorrect will be achieved through transformer-based classifiers (BERT, SciBERT, PubMedBERT), which will be evaluated against human-coded ground truth data. In its final form, the tool will serve as a writing assistant, a research instrument for large-scale corpus analysis, and an extension of papercheck. Ultimately, the goal is to reduce misinterpretations of nonsignificant findings and contribute to more accurate and informative scientific reporting."
keywords: [p value, misinterpretation, automation, automated checks, RegEx, LLMs, BERT]
floatsintext: true
numbered-lines: false
mask: false # change for blind peer review
bibliography: references.bib
format:
  apaquarto-html: default
  apaquarto-pdf:
    documentmode: man
  apaquarto-typst: default
  apaquarto-docx: default
a4paper: true
---

```{r load libraries}
#| include: false
library(papercheck)
library(readxl)
library(psych)
library(tidyverse)
library(flextable)
library(papaja)
```

# 1 Introduction

Over the past decades, numerous articles have addressed common misinterpretations of *p* values in the context of standard null hypothesis significance testing (NHST) [@goodman08; @greenland_etal16; @schervish96]. Some go further, questioning the use of frequentist methods altogether [e.g., @edwards_etal63; @wagenmakers07], while others propose refinements within the frequentist framework that aim to improve the informativeness of statistical inference [e.g., @isager_fitzgerald24; @lakens_etal18]. If you are a researcher writing a paper and want to interpret your results correctly, the solution seems simple: read these educational resources and revise your manuscript accordingly. Easy, right? Still, empirical studies consistently show that these misinterpretations remain widespread [e.g., @hoekstra_etal06; @murphy_etal25]. Why is that? What makes interpreting *p* values so persistently difficult? Which practical solutions or promising approaches might help? And are some of the proposed 'misinterpretation checklists' perhaps less informative than their authors would hope?

In this master's thesis, I show how rule-based approaches, combined with natural language processing (NLP) can be used to automatically detect, classify, and correct these misinterpretation. I show this for the misinterpretation of statistically nonsignificant results as the absence of an effect because it is the most extensively researched misinterpretation of *p* values [@lakens21], and I have experience in classifying them from a previous project [@murphy_etal25]. That said, the general framework I propose can be easily adapted to address other misinterpretations, also beyond *p* values.

## 1.1 Common Misinterpretations of *p* Values

The criticism of *p* values has become a prominent and recurring theme in discussions around scientific reform. From claims that they encourage dichotomous thinking [@hoekstra_etal06; @amrhein_etal19] to arguments that they offer little informational value [@wagenmakers07], *p* values – and the broader framework of NHST – have been blamed for many of science’s replication problems [@mcshane_etal19]. On the other hand, many have also argued that NHST per se is not to blame for these problems, but rather how researchers (mis)use and (mis)interpret this tool [@lakens21]. As a result, many researchers present whole collections of, in their view, common *p* value misinterpretations [see, e.g., @goodman08; @greenland_etal16]. Reviewing these, I come to the conclusion that there are four distinct types of misconceptions about *p* values that seem to be at play:

XXX

In my master thesis, I will zoom in on one specific misinterpretation: concluding *no effect* based on a statistically nonsignificant finding. Many studies have previously shown that this misinterpretation is and remains highly prevalent across time and sub-domains of psychology [e.g., @aczel_etal18; @hoekstra_etal06; @murphy_etal25]. In fact, in a recently published article investigating articles published in 2009, 2015, and 2021 across ten different psychology journals (mainly in the field of personality and social psychology), we estimated the prevalence of this misinterpretation in articles’ discussion sections to lie between 76.17% and 84.90% [@murphy_etal25]. This study highlights that the situation seems not to have greatly improved despite many researchers exploring new analysis techniques [e.g., @lakens_etal18] and continuous calls to reflect on interpretations of nonsignificant results [e.g., @mcshane_etal19].

### 1.1.1 The Big-Four *p* value misinterpretations

In preparation for this thesis project, I reviewed many of the previously reported misinterpretations of *p* values [e.g., @goodman08; @greenland_etal16; chapter 1.7 from @lakens24a] and categorize them into four main groups:

-   *p* values as hypothesis probabilities
-   Blending statistical and practical significance
-   *p* values as measures of replicability or error rates
-   Technical misunderstandings about *p* values

I argue that many of these published 'misinterpretation checklists' largely reiterate similar underlying misconceptions, often merely rephrasing what is, at its core, the same fundamental issue. While I will elaborate on this reasoning in the final thesis more, for the purposes of this proposal, I focus on the first category.

This misinterpretation refers to the tendency of researchers to treat *p* values as if they represented the probability that the null (or alternative) hypothesis is true. Researchers who follow this misinterpretation may interpret *p* values below the conventional 5% threshold as evidence that $H_1$ is true (and $H_0$ is false), and nonsignificant *p* values as evidence that $H_0$ is true (and $H_1$ is false). In this project, I specifically focus on the latter mistake: interpreting a nonsignificant result as proof that no effect exists. This interpretation cannot be justified within the standard NHST framework, which defines the *p* value as the probability of observing the data, or something more extreme, assuming that the null hypothesis is true. There are, however, ways to overcome these misinterpretations, which I will discuss in the next section.

## 1.2 Research on nonsignificance as absence

XXX @aczel_etal18; @hoekstra_etal06; @murphy_etal25; and more

## 1.3 Possible solutions

This section will be more detailed in the final thesis, but I do want to briefly outline what I consider the most important solutions to the misinterpretation of nonsignificant results as evidence for the absence of an effect. One frequently suggested solution is to improve researchers’ statistical literacy through enhanced education, such as better statistics teaching at the undergraduate and graduate levels [e.g., @lakens21]. However, as noted earlier, the prevalence of the misinterpretations I focus on does not seem to have substantially decreased, suggesting that calls for better education alone have not resolved the problem [@murphy_etal25].

A promising practical solution when conducting research involves the use of alternative analysis techniques, such as equivalence testing or minimum-effect tests. These methods allow researchers to test whether an effect is practically relevant and larger than a predefined smallest effect size of interest (SESOI) [@lakens_etal18]. In many contexts, such approaches might be more closely aligned with the substantive questions researchers aim to answer, namely whether an effect is meaningful in practice.

## 1.4 An Automated Human-Factors Perspective

These strategies also align with the argument made by @lakens21 that *p* value misinterpretations represent a human factors problem, requiring practical and easy-to-implement solutions. Everyday examples of such solutions include cars with automatic braking systems, word processors that flag spelling and grammar mistakes, or email clients that filter out malware and phishing attempts. Analogously, and recognizing that new analytic approaches may not be adopted overnight, automated checks for statistical misinterpretations offer a highly promising route. This perspective emphasizes that many statistical errors arise not from bad intentions or ignorance, but from cognitive limitations and suboptimal workflows.

In the context of research, similar automated solutions are already gaining traction. For instance, the reference manager Zotero flags references to retracted papers [@stillman19]. Statcheck [@nuijten_epskamp24] automatically detects inconsistencies between reported test statistics and *p* values. Other tools, such as GRIM, GRIMMER, and SPRITE, identify impossible values in reported summary statistics [@heathers_etal18], while Regcheck [@cummin_hussey24] verifies the consistency between manuscripts and their preregistration documents.

To make the process of checking manuscripts more systematic, @R-papercheck developed papercheck, an R package and Shiny app, which allows users to run a battery of checks on research papers. These include statistical checks (e.g., identifying imprecisely reported *p* values) as well as general manuscript quality checks (e.g., verifying links to online repositories or consistency between in-text citations and reference lists). Papercheck can be used both for single articles (e.g., as writing assistance) and for batches of articles (e.g., for meta-scientific studies). Because this framework is actively maintained and continues to evolve, I plan to build my thesis project within the papercheck infrastructure.

In summary, there are many reasons why *p* values remain difficult to interpret correctly. Empirical evidence suggests that misinterpretations of nonsignificant results remain highly prevalent [@murphy_etal25] This persistence highlights that improved education alone may not be sufficient. Drawing on a human factors perspective [@lakens21], practical solutions such as automated error-checking tools offer a promising avenue for addressing these challenges. In this project, I aim to develop an automated approach to detect misinterpretations of nonsignificant results, building on the existing papercheck framework [@R-papercheck]. In the following section, I outline the methods and approaches that I will explore to achieve this goal.

# 2 Methods

## 2.1 Statement Detection, Classification and Correction

Before describing the data used in this study, it is important to understand the three steps of the proposed framework. Statements from scientific articles needed to be reliably detected, classified, and finally corrected. For each step, I applied specific methods that were best suited to achieve the respective goal.

To detect statements I searched used rule-based regular expressions (RegEx) and searched articles's results sections for them. Effectively, RegEx searchers are just more complex Ctrl+F searches, where a user can also include optional charcters (e.g., 'significant(ly)' would catch both *significant* and *significantly*) and more complex rules (e.g., 'not.{0,20}significant' allows up to 20 characters between *not* and *significant*). Papercheck [@R-papercheck] has a module that detects almost all p values (see Section XXX) based on RegEx searches and I filtered these to just the ones equal to or above .05. I then expanded the extracted nonsignificant p values to the full sentence with papercheck and added +/- one sentence as context in case of extraction errors etc. (more on this in Section XXX).[^1]

[^1]: In a final tool, users will be able to set the alpha level they used themselves, thus allowing other levels than the conventional 5%.

In the next step, these statements (labeled as correct or incorrect by me; see Section XXX) were used to train several BERT-based models. BERT (Bidirectional Encoder Representations from Transformers) is a general-purpose language model pre-trained on the BookCorpus and English Wikipedia, making it suitable for a wide range of tasks – but not specifically optimized for scientific or technical language [@devlin_etal19]. Since its introduction, many researchers have developed domain-specific variants of BERT to enhance its performance on specialized tasks. To test whether such domain adaptation improves performance in my classification task, I trained two models in addition to standard BERT: SciBERT was trained on a large corpus of scientific articles from Semantic Scholar, particularly in the biomedical and computer science domains [@beltagy_etal19]. PubMedBERT goes even further, having been trained exclusively on biomedical abstracts and full-text articles from the PubMed database [@gu_etal22]. These models were evaluated on their ability to distinguish between correct and incorrect interpretations of nonsignificant results in scientific writing.

Lastly, statements that have been labeled as being incorrect by a BERT classifier were sent to a LLM to get corrected. The full prompt is available in X was had X word plus the respective statement. In short, the model was instructed to only change any misinterpretations of nonsignificant p values as the absence of an effect and keep the rest of the statement. Do communicate with the LLM, I used papercheck [@R-papercheck], which, in turn, uses the Groq API (available at <https://groq.com/>) to communicate with different LLMs. I used papercheck's standard LLM, 'llama-3.3-70b-versatile' (as of 07/24/2025).

## 2.2 Validation Process and Performance Metrics

```{r papercheck sample library descriptives}
#| include: false
#### Descriptive of the Sample Articles
sample_data <- papercheck::psychsci

submission_years <- c()

# Extract all submission strings
for (article in sample_data) {
  submission <- article[["info"]][["submission"]][[1]]
  submission_years <- c(submission_years, submission)
}

submission_df <- data.frame(submission_years, stringsAsFactors = FALSE)

# Extract the date following "Revision accepted"
submission_df$revision_accepted_date <- sub(
  ".*(?:Revision accepted|Accepted)[ ]*([0-9]{1,2}/[0-9]{1,2}/[0-9]{2}).*",
  "\\1",
  submission_df$submission_years
)

# Clean and normalize dates (pad month/day if needed)
split_dates <- strsplit(submission_df$revision_accepted_date, "/")

normalized_dates <- sapply(split_dates, function(x) {
  if (length(x) == 3) {
    month <- sprintf("%02d", as.numeric(x[1]))
    day   <- sprintf("%02d", as.numeric(x[2]))
    year  <- x[3]
    paste0(month, "/", day, "/", year)
  } else {
    NA
  }
})

# Convert to Date and extract year
submission_df$date_parsed <- as.Date(normalized_dates, format = "%m/%d/%y")
submission_df$publication_year <- format(submission_df$date_parsed, "%Y")

# Convert publication_year to numeric, in case it's still a character
submission_df$publication_year <- as.numeric(submission_df$publication_year)

# Get full descriptive stats
descriptives_article_year <- describe(submission_df$publication_year)

# Extract and round common statistics
median_article_year  <- round(descriptives_article_year$median, 0)
min_article_year     <- round(descriptives_article_year$min, 0)
max_article_year     <- round(descriptives_article_year$max, 0)

# Get Q1 and Q3 using base R
quartiles <- quantile(submission_df$publication_year, probs = c(0.25, 0.75), na.rm = TRUE)
q1_article_year <- round(quartiles[[1]], 0)
q3_article_year <- round(quartiles[[2]], 0)
```

MAYBE (RE)MOVE The articles that were used in this study were part of papercheck's sample library of 250 open access article from the journal Psychological Science, published between `r min_article_year` and `r max_article_year` (Median = `r median_article_year`; IQA = \[`r q1_article_year`; `r q3_article_year`\]). MAYBE (RE)MOVE

To assess how well each of these three automated approaches worked, I compared each one to human ground truth and calculated appropriate measures of reliability between automated and human results.

Firstly, to ensure that the statement detection process actually caught all statements with nonsignificant p values, I manually extracted all of these from 25 (10%) of the papercheck sample library's 250 open access article from the journal Psychological Science. Articles were published between `r min_article_year` and `r max_article_year` (Median = `r median_article_year`; IQA = \[`r q1_article_year`; `r q3_article_year`\]). I then coded whether a statements I found were also extracted with the automated RegEx search.

```{r labeled data descriptives}
#| include: false
labeled_data <- read_excel("../data/training_data/labeled/labeled_data.xlsx")

labeled_results <- labeled_data[labeled_data$section == "results",]
labeled_results_99 <- labeled_results[labeled_results$label == -99,]
labeled_results_0 <- labeled_results[labeled_results$label == 0,]
labeled_results_1 <- labeled_results[labeled_results$label == 1,]

keywords_99 <- table(labeled_results_99$label_keywords)

keywords_99_flaseflag <- keywords_99[['false flag']]+keywords_99[['figure note']]
keywords_99_incomplete <- keywords_99[['incomplete']]+keywords_99[['more context needed']]
keywords_99_marginally <- keywords_99[['marginally significant']]+keywords_99[['incorrect other; marginally significant']]
keywords_99_modelfit <- keywords_99[['incomplete']]+keywords_99[['model fit; incomplete']]
```

For the training of the BERT models and to assess their final performance I labeled all automatically extracted statements that were detected to be from an article's Results section. This resulted in `r nrow(labeled_results)` statements in total. Of these, `r nrow(labeled_results_0)` were classified as correct and `r nrow(labeled_results_1)` were classified as incorrect. The remaining `r nrow(labeled_results_99)` statements were classified as not neither correct nor incorrect because they interpreted the nonsignificant effect as (marginally) significant (`r keywords_99_marginally`), because the statements were not complete enough to check their correctness (`r keywords_99_incomplete`), because they interpreted model fit indices and not the p value (`r keywords_99_modelfit`), because they were 'false flags' of nonsignificant p values (`r keywords_99_flaseflag`), or due to a combination of these or other reasons (`r nrow(labeled_results_99)-keywords_99_flaseflag-keywords_99_incomplete-keywords_99_marginally-keywords_99_modelfit`).

Lastly, I also also went through the 100 statements that were sent to an LLM to be corrected, to see if the 'corrected' statements were actually correct. Of these 100 statements, 80 had previously been labeled incorrect and 20 correct by me, to check how the LLM deals with possible false positives from the automated classification.

In addition to these validity checks, there are also performance metrics specific to the trained classifiers. For training purposes, the labeled data was split into three parts: a test set (20%) used for the final evaluation of the best model; a training set (72%, or 90% of the remaining 80%) that the model uses to learn underlying patterns and adjust its parameters; and a validation set (8%, or 10% of the 80%) used to calculate evaluation metrics after each epoch (i.e., one full cycle of the model processing the training data).

During BERT training, I computed the training loss (sum of errors between model predictions and actual labels in the training set) and the validation loss (same for validation set). The best-performing model was selected based on the lowest validation loss. The model would have been trained on a maximum of 10 epochs, but training ended early if the model did not improve, as measured by the validation loss, for two epochs. For final evaluation, I computed the fraction of correctly predicted positive cases among all predicted positives (precision), the fraction of correctly predicted positive cases among all actual positives (recall), and their harmonic mean (F1 score), separately for each class label. To summarize overall performance across the two classes, I calculated the unweighted average of the two F1 scores (macro-F1 score).

## 2.4 Software

All scripts for this thesis project were written in R [Version 4.5.0\; @rcoreteam25] or Python [Version 3.12.10\; @python].

In R, I used *papercheck* [Version 0.0.0.9049\; @R-papercheck] for accessing the 250 open access articles, preprocess them and for communication with the LLM, *readxl* [Version 1.4.5\; @R-readxl] to access Excel files in R, *psych* [Version 2.5.6\; @R-psych] for calculating descriptive statistics, *tidyverse* [Version 2.0.0\; @R-tidyverse] for data preprocessing and visualization, and *flextable* [Version 0.9.9\; @R-flextable], *papaja* [Version 0.1.3\; @R-papaja] and *showtext* [Version 0.9-7\; @R-showtext] to create APA-formatted tables.

The Python libraries used to train the BERT models can be found in this requirements file: LINK.

All scripts and data to reproduce and use the trained BERT models (Python), analyse the results and validity checks (mostly R) and recreate this manuscript (Quarto Markdown in R Studio; I did change few formatting, not content, related things manually in the exported Word document) are available in this GitHub repository, together with instructions on how to set it up: LINK.

This thesis was not preregistered as no inferential statistical tests were performed.

# 3 Results

## 3.1 Detection Accuracy

```{r detection_check data}
#| include: false

detection_check <- read_excel("../data/detection_check/detection_checked.xlsx")

agreement_table <- table(detection_check$agreement)
agreement_1 <- agreement_table[['1']]
agreement_2 <- agreement_table[['2']]
agreement_3 <- agreement_table[['3']]
agreement_3_FP <- sum(detection_check$agreement == 3 & detection_check$agree_3_FP_FN == "FP", na.rm = TRUE)
agreement_3_FN <- sum(detection_check$agreement == 3 & detection_check$agree_3_FP_FN == "FN", na.rm = TRUE)

keywords_table <- table(detection_check$disagreement_keyword)
keywords_figure <- keywords_table[['figure note']]
keywords_footnote <- keywords_table[['footnote']]
keywords_formatting <- keywords_table[['formatting']]
keywords_character <- keywords_table[['not sure; unusual character']]
keywords_pagebreak <- keywords_table[['page break']]
keywords_tablecontent <- keywords_table[['table content']]
keywords_ns <- keywords_table[['n.s.']]
keywords_ps <- keywords_table[['p_s']]
```

Manually going through 25 articles from papercheck's sample library I detected `r agreement_1+agreement_2+agreement_3_FN` statements with a nonsignificant p value in total. The automated RegEx searches caught `r agreement_1` (`r round(100*(agreement_1/(agreement_1+agreement_2+agreement_3_FN)))` %) of these completely, and `r agreement_3_FN` partially (incompletely) due to extraction errors (e.g., because of pdf formatting like page breaks, figures or footnotes). It also 'found' `r agreement_3_FP` false positives in the sense that it extracted 'statements' from tables or figure notes or ones that were false classified as coming from a results section. Note, however, that the large majority of the total `r agreement_2+agreement_3_FN` missed statements were due to specific ways of writing (or not writing) the p value: `r keywords_ns` were missed because the authors wrote 'n.s.' instead of the nonsignificant p value, and `r keywords_ps` were missed because the p value was written as '$p_s$'. Without these two mistakes the overall agreement of automated and manual approach would have been `r round(100*(agreement_1/(agreement_1+agreement_2+agreement_3_FN-keywords_ns-keywords_ps)))` %.

Most of the other misses were due to pdf formatting issues like figures, tables, footnotes and page breaks or unusual characters inside the statement that messed with the statement extraction (`r keywords_figure+keywords_footnote+keywords_formatting+keywords_character+keywords_pagebreak+keywords_tablecontent` in total).[^2]

[^2]: I could not find one statement that was extracted automatically. My current theory is that this was an artifact from when the pdf was compiled and might be from a different article even, once again highlighting how impractical the pdf format is in times of increasing automation.

## 3.2 Classification Performance

```{r BERT performance}
#| include: false
#################################
#####                       #####
#####     Standard BERT     #####
#####                       #####
#################################

#### Load data
classification_report_BERT <- read.csv("../data/model_performance/classification_report/BERT.csv")
training_history_BERT <- read.csv("../data/model_performance/model_training_history/BERT.csv")
test_predictions_BERT <-read.csv("../data/model_performance/test_predictions/BERT.csv")

#### Data preparation
  # Training History

# Fill NAs in selected columns using the value from the previous row
training_history_BERT <- training_history_BERT |>
  fill(loss, grad_norm, learning_rate, .direction = "down")

# Remove odd-numbered rows
training_history_BERT <- training_history_BERT[seq(2, nrow(training_history_BERT), by = 2), ]

#### Preparing inline citation
  # Classification report
precision_0_BERT <- apa_p(classification_report_BERT$precision[1], 2)
precision_1_BERT <- apa_p(classification_report_BERT$precision[2], 2)
recall_0_BERT <- apa_p(classification_report_BERT$recall[1], 2)
recall_1_BERT <- apa_p(classification_report_BERT$recall[2], 2)
F1_0_BERT <- apa_p(classification_report_BERT$f1.score[1], 2)
F1_1_BERT <- apa_p(classification_report_BERT$f1.score[2], 2)

macro_F1_BERT <- apa_p(classification_report_BERT$f1.score[4], 2)

n_total <- round(classification_report_BERT$support[5], 0)

  # Training History
n_epochs_BERT <- max(training_history_BERT$epoch)
best_epoch_BERT <- training_history_BERT$epoch[training_history_BERT$eval_loss == min(training_history_BERT$eval_loss)]

#################################
#####                       #####
#####        SciBERT        #####
#####                       #####
#################################

#### Load data
classification_report_SciBERT <- read.csv("../data/model_performance/classification_report/SciBERT.csv")
training_history_SciBERT <- read.csv("../data/model_performance/model_training_history/SciBERT.csv")
test_predictions_SciBERT <-read.csv("../data/model_performance/test_predictions/SciBERT.csv")

#### Data preparation
  # Training History

# Fill NAs in selected columns using the value from the previous row
training_history_SciBERT <- training_history_SciBERT |>
  fill(loss, grad_norm, learning_rate, .direction = "down")

# Remove odd-numbered rows
training_history_SciBERT <- training_history_SciBERT[seq(2, nrow(training_history_SciBERT), by = 2), ]

#### Preparing inline citation
  # Classification report
precision_0_SciBERT <- apa_p(classification_report_SciBERT$precision[1], 2)
precision_1_SciBERT <- apa_p(classification_report_SciBERT$precision[2], 2)
recall_0_SciBERT <- apa_p(classification_report_SciBERT$recall[1], 2)
recall_1_SciBERT <- apa_p(classification_report_SciBERT$recall[2], 2)
F1_0_SciBERT <- apa_p(classification_report_SciBERT$f1.score[1], 2)
F1_1_SciBERT <- apa_p(classification_report_SciBERT$f1.score[2], 2)

macro_F1_SciBERT <- apa_p(classification_report_SciBERT$f1.score[4], 2)

n_total <- round(classification_report_SciBERT$support[5], 0)

  # Training History
n_epochs_SciBERT <- max(training_history_SciBERT$epoch)
best_epoch_SciBERT <- training_history_SciBERT$epoch[training_history_SciBERT$eval_loss == min(training_history_SciBERT$eval_loss)]

#################################
#####                       #####
#####      PubMedBERT       #####
#####                       #####
#################################

#### Load data
classification_report_PubMedBERT <- read.csv("../data/model_performance/classification_report/PubMedBERT.csv")
training_history_PubMedBERT <- read.csv("../data/model_performance/model_training_history/PubMedBERT.csv")
test_predictions_PubMedBERT <-read.csv("../data/model_performance/test_predictions/PubMedBERT.csv")

#### Data preparation
  # Training History

# Fill NAs in selected columns using the value from the previous row
training_history_PubMedBERT <- training_history_PubMedBERT |>
  fill(loss, grad_norm, learning_rate, .direction = "down")

# Remove odd-numbered rows
training_history_PubMedBERT <- training_history_PubMedBERT[seq(2, nrow(training_history_PubMedBERT), by = 2), ]

#### Preparing inline citation
  # Classification report
precision_0_PubMedBERT <- apa_p(classification_report_PubMedBERT$precision[1], 2)
precision_1_PubMedBERT <- apa_p(classification_report_PubMedBERT$precision[2], 2)
recall_0_PubMedBERT <- apa_p(classification_report_PubMedBERT$recall[1], 2)
recall_1_PubMedBERT <- apa_p(classification_report_PubMedBERT$recall[2], 2)
F1_0_PubMedBERT <- apa_p(classification_report_PubMedBERT$f1.score[1], 2)
F1_1_PubMedBERT <- apa_p(classification_report_PubMedBERT$f1.score[2], 2)

macro_F1_PubMedBERT <- apa_p(classification_report_PubMedBERT$f1.score[4], 2)

n_total <- round(classification_report_PubMedBERT$support[5], 0)

  # Training History
n_epochs_PubMedBERT <- max(training_history_PubMedBERT$epoch)
best_epoch_PubMedBERT <- training_history_PubMedBERT$epoch[training_history_PubMedBERT$eval_loss == min(training_history_PubMedBERT$eval_loss)]
```

@fig-2 shows the training and validation loss of the three BERT models over their training. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

```{r Combine model loss curves into one figure}
#| include: false
library(magick)

# Read all three PDFs (first page of each)
loss_curve_bert     <- image_read_pdf("../data/model_performance/loss_curve/BERT.pdf")
loss_curve_scibert  <- image_read_pdf("../data/model_performance/loss_curve/SciBERT.pdf")
loss_curve_pubmed   <- image_read_pdf("../data/model_performance/loss_curve/PubMedBERT.pdf")

# Combine them horizontally
combined <- image_append(c(loss_curve_bert, loss_curve_scibert, loss_curve_pubmed))

# Save to a new PDF
image_write(combined, path = "../data/model_performance/loss_curve/loss_combined.pdf", format = "pdf")
```

![Training and Validation Loss Curve](../data/model_performance/loss_curve/loss_combined.pdf){#fig-2 width="6in" fig-alt="Curves of the training and validation loss of the three trained BERT models. The best models for regular BERT, SciBERT and PubMedBERT were chosen after epoch ´r best_epoch_BERT´, ´r best_epoch_SciBERT´, and ´r best_epoch_PubMedBERT´ respectively, based on the minumum validation loss." apa-note="Curves of the training and validation loss of the three trained BERT models. The best models for regular BERT, SciBERT and PubMedBERT were chosen after epoch ´r best_epoch_BERT´, ´r best_epoch_SciBERT´, and ´r best_epoch_PubMedBERT´ respectively, based on the minumum validation loss."}

The different performance metrics can be found in @tbl-model-performance. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

```{r}
#| label: tbl-model-performance
#| message: false
#| warning: false
#| apa-note: Table of precision, recall and F1 score per model and class.
#| ft-align: left
#| tbl-cap: Model Performance

# Create the data
tbl <- tibble(
  ' '            = c("Correct Class", "Incorrect Class", "Macro F1 score"),
  'BERT 1'       = c(precision_0_BERT, precision_1_BERT, ""),
  'BERT 2'       = c(recall_0_BERT, recall_1_BERT, ""),
  'BERT 3'       = c(F1_0_BERT, F1_1_BERT, macro_F1_BERT),
  'SciBERT 1'    = c(precision_0_SciBERT, precision_1_SciBERT, ""),
  'SciBERT 2'    = c(recall_0_SciBERT, recall_1_SciBERT, ""),
  'SciBERT 3'    = c(F1_0_SciBERT, F1_1_SciBERT, macro_F1_SciBERT),
  'PubMedBERT 1' = c(precision_0_PubMedBERT, precision_1_PubMedBERT, ""),
  'PubMedBERT 2' = c(recall_0_PubMedBERT, recall_1_PubMedBERT, ""),
  'PubMedBERT 3' = c(F1_0_PubMedBERT, F1_1_PubMedBERT, macro_F1_PubMedBERT)
)

# Create the flextable with grouped headers
tbl %>%
  flextable() %>%
  set_header_labels(
    `BERT 1` = "Precision", `BERT 2` = "Recall", `BERT 3` = "F1 score",
    `SciBERT 1` = "Precision", `SciBERT 2` = "Recall", `SciBERT 3` = "F1 score",
    `PubMedBERT 1` = "Precision", `PubMedBERT 2` = "Recall", `PubMedBERT 3` = "F1 score"
  ) %>%
  add_header_row(
  values = c(" ", "BERT", "SciBERT", "PubMedBERT"),
  colwidths = c(1, 3, 3, 3)
  ) %>%
  flextable::fit_to_width(max_width = 6.5) %>%
  fontsize(size = 11, part = "all") %>%
  flextable::theme_apa()
```

```{r Combine model confusion matrices into one figure}
#| include: false
library(magick)

# Read all three PDFs (first page of each)
confusion_matrix_bert     <- image_read_pdf("../data/model_performance/confusion_matrix/BERT.pdf")
confusion_matrix_scibert  <- image_read_pdf("../data/model_performance/confusion_matrix/SciBERT.pdf")
confusion_matrix_pubmed   <- image_read_pdf("../data/model_performance/confusion_matrix/PubMedBERT.pdf")

# Combine them horizontally
combined <- image_append(c(confusion_matrix_bert, confusion_matrix_scibert, confusion_matrix_pubmed))

# Save to a new PDF
image_write(combined, path = "../data/model_performance/confusion_matrix/confusion_matrix_combined.pdf", format = "pdf")
```

![Confusion Matrix](../data/model_performance/confusion_matrix/confusion_matrix_combined.pdf){#fig-3 width="6.5in" fig-alt="Confusion matrices of the three trained BERT models." apa-note="Confusion matrices of the three trained BERT models."}

Examples of incorrectly classified statements: Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet. Lorem ipsum dolor sit amet, consetetur sadipscing elitr, sed diam nonumy eirmod tempor invidunt ut labore et dolore magna aliquyam erat, sed diam voluptua. At vero eos et accusam et justo duo dolores et ea rebum. Stet clita kasd gubergren, no sea takimata sanctus est Lorem ipsum dolor sit amet.

## 3.3 Correction Evaluation

```{r}
#| include: false
llm_corrections <- read_excel("../data/llm_correction_check/llm_correction_checked.xlsx")

n_correct <- sum(llm_corrections$corrected_correct == 0)
n_incorrect <- sum(llm_corrections$corrected_correct == 1)

n_0_correct <- sum(llm_corrections$label == 0 & llm_corrections$corrected_correct == 0)
n_0_incorrect <- sum(llm_corrections$label == 0 & llm_corrections$corrected_correct == 1)

n_1_correct <- sum(llm_corrections$label == 1 & llm_corrections$corrected_correct == 0)
n_1_incorrect  <- sum(llm_corrections$label == 1 & llm_corrections$corrected_correct == 1)
```

Of the 100 statements that the LLM was instructed to correct `r n_correct` were correct. Interestingly, `r n_0_incorrect` of the 20 already correct statements got turned incorrect by the LLM. `r n_0_correct`, on the other hand, remained correct. Similarly, the LLM actually corrected `r n_1_correct` of the 80 incorrect statements, whereas `r n_1_incorrect` remained incorrect. Note, however, that the LLM was instructed to change (as much as necessary, but) as little as possible about the original statement. For some statements, this meant that they could not be corrected without major rephrasing. Examples for some bad/good corrections can be found in @tbl-LLM-corrections-1 and @tbl-LLM-corrections-2, respectively.

```{r}
#| label: tbl-LLM-corrections-1
#| message: false
#| warning: false
#| apa-note: Table of original and LLM-corrected statements that were classified as incorrect. In the examples '0' refers to correct and '1' to incorrect.
#| ft-align: left
#| tbl-cap: Examples of Incorrect LLM Corrections

# Your selected example indices
examples <- c(60, 80)

# Fix 'ÃŽÂ²' in statements --> should be beta
llm_corrections$statement <- gsub("ÃŽÂ²|ÃŽÂ²", "β", llm_corrections$statement)
llm_corrections$corrected <- gsub("ÃŽÂ²|ÃŽÂ²", "β", llm_corrections$corrected)

# Create long format table
df_long <- map_dfr(examples, function(i) {
  example_label <- paste0("Example ", llm_corrections$label[i], " to ", llm_corrections$corrected_correct[i])
  
  tibble(
    Example = rep(example_label, 2),
    `Statement Type` = c("Original", "LLM-Corrected"),
    Statement = c(llm_corrections$statement[i], llm_corrections$corrected[i]),
    Label = as.character(c(llm_corrections$label[i], llm_corrections$corrected_correct[i]))
  )
})

# Create the table with merged 'Example' cells
df_long %>%
  flextable() %>%
  flextable::theme_apa() %>%
  merge_v(j = ~Example) %>%       # Merge vertical cells in the 'Example' column
  valign(j = ~Example, val = "center") %>%  # Align merged cells at the top
  line_spacing(j = c(1:3), space = 1.2) %>%  # Set line spacing in Statement column
  # fontsize(size = 9, part = "all") %>%
  width(j = 1, width = 0.7) %>%   # Set width of column 1 (Example)
  width(j = 2, width = 0.7) %>%   # Column 2 (Statement Type)
  width(j = 3, width = 4) %>%   # Column 3 (Statement)
  width(j = 4, width = 0.7) #%>%   # Column 4 (Label)
  # autofit() %>%
```

```{r}
#| label: tbl-LLM-corrections-2
#| message: false
#| warning: false
#| apa-note: Table of original and LLM-corrected statements that were classified as incorrect. In the examples '0' refers to correct and '1' to incorrect.
#| ft-align: left
#| tbl-cap: Examples of Correct LLM Corrections

# Your selected example indices
examples <- c(10, 79)

# Fix 'ÃŽÂ²' in statements --> should be beta
llm_corrections$statement <- gsub("ÃŽÂ²|ÃŽÂ²", "β", llm_corrections$statement)
llm_corrections$corrected <- gsub("ÃŽÂ²|ÃŽÂ²", "β", llm_corrections$corrected)

# Create long format table
df_long <- map_dfr(examples, function(i) {
  example_label <- paste0("Example ", llm_corrections$label[i], " to ", llm_corrections$corrected_correct[i])
  
  tibble(
    Example = rep(example_label, 2),
    `Statement Type` = c("Original", "LLM-Corrected"),
    Statement = c(llm_corrections$statement[i], llm_corrections$corrected[i]),
    Label = as.character(c(llm_corrections$label[i], llm_corrections$corrected_correct[i]))
  )
})

# Create the table with merged 'Example' cells
df_long %>%
  flextable() %>%
  flextable::theme_apa() %>%
  merge_v(j = ~Example) %>%       # Merge vertical cells in the 'Example' column
  valign(j = ~Example, val = "center") %>%  # Align merged cells at the top
  line_spacing(j = c(1:3), space = 1.2) %>%  # Set line spacing in Statement column
  # fontsize(size = 9, part = "all") %>%
  width(j = 1, width = 0.7) %>%   # Set width of column 1 (Example)
  width(j = 2, width = 0.7) %>%   # Column 2 (Statement Type)
  width(j = 3, width = 4) %>%   # Column 3 (Statement)
  width(j = 4, width = 0.7) #%>%   # Column 4 (Label)
  # autofit() %>%
```

# 4 Discussion

## 4.1 Summary of Findings

## 4.2 Strengths of the Approach

## 4.3 Limitations and Challenges

## 4.4 Implication of the Tool

## 4.5 Future Directions and Improvements

# 5 Conclusion

# References

<!-- References will auto-populate in the refs div below -->

::: {#refs}
:::

# Appendix

XXX
