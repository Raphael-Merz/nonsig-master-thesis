@article{aczel_etal18,
  title = {Quantifying {{Support}} for the {{Null Hypothesis}} in {{Psychology}}: {{An Empirical Investigation}}},
  shorttitle = {Quantifying {{Support}} for the {{Null Hypothesis}} in {{Psychology}}},
  author = {Aczel, Balazs and Palfi, Bence and Szollosi, Aba and Kovacs, Marton and Szaszi, Barnabas and Szecsi, Peter and Zrubka, Mark and Gronau, Quentin F. and Van Den Bergh, Don and Wagenmakers, Eric-Jan},
  year = {2018},
  month = sep,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {3},
  pages = {357--366},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/2515245918773742},
  urldate = {2025-03-10},
  abstract = {In the traditional statistical framework, nonsignificant results leave researchers in a state of suspended disbelief. In this study, we examined, empirically, the treatment and evidential impact of nonsignificant results. Our specific goals were twofold: to explore how psychologists interpret and communicate nonsignificant results and to assess how much these results constitute evidence in favor of the null hypothesis. First, we examined all nonsignificant findings mentioned in the abstracts of the 2015 volumes of Psychonomic Bulletin \& Review, Journal of Experimental Psychology: General, and Psychological Science ( N = 137). In 72\% of these cases, nonsignificant results were misinterpreted, in that the authors inferred that the effect was absent. Second, a Bayes factor reanalysis revealed that fewer than 5\% of the nonsignificant findings provided strong evidence (i.e., BF               01               {$>$} 10) in favor of the null hypothesis over the alternative hypothesis. We recommend that researchers expand their statistical tool kit in order to correctly interpret nonsignificant results and to be able to evaluate the evidence for and against the null hypothesis.},
  langid = {english},
  keywords = {prevalences,previous research},
  file = {C\:\\Users\\raphi\\OneDrive\\RUB\\1_Master\\Masterarbeit\\3_Writing\\References\\aczel_etal18.pdf;C\:\\Users\\raphi\\Zotero\\storage\\HBSAYX6Y\\Aczel et al_2018_Quantifying Support for the Null Hypothesis in Psychology.pdf}
}

@article{amrhein_etal19,
  title = {Scientists Rise up against Statistical Significance},
  author = {Amrhein, Valentin and Greenland, Sander and McShane, Blake},
  year = {2019},
  month = mar,
  journal = {Nature},
  volume = {567},
  number = {7748},
  pages = {305--307},
  issn = {0028-0836, 1476-4687},
  doi = {10.1038/d41586-019-00857-9},
  urldate = {2025-04-16},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  langid = {english},
  file = {C:\Users\raphi\Zotero\storage\8MFX4AT3\Amrhein et al. - 2019 - Scientists rise up against statistical significance.pdf}
}

@article{badenes-ribera_etal15,
  title = {Interpretation of the p Value: {{A}} National Survey Study in Academic Psychologists from {{Spain}}},
  shorttitle = {Interpretation of the p Value},
  author = {{Badenes-Ribera}, Laura and {Fr{\'i}as-Navarro}, Dolores and {Monterde-i-Bort}, H{\'e}ctor and {Pascual-Soler}, Marcos},
  year = {2015},
  month = aug,
  journal = {Psicothema},
  volume = {3},
  number = {27},
  pages = {290--295},
  issn = {0214-9915, 1886-144X},
  doi = {10.7334/psicothema2014.283},
  urldate = {2025-04-17}
}

@article{badenes-ribera_etal16,
  title = {Misconceptions of the P-Value among {{Chilean}} and {{Italian Academic Psychologists}}},
  author = {{Badenes-Ribera}, Laura and {Frias-Navarro}, Dolores and Iotti, Bryan and {Bonilla-Campos}, Amparo and Longobardi, Claudio},
  year = {2016},
  month = aug,
  journal = {Frontiers in Psychology},
  volume = {7},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2016.01247},
  urldate = {2025-04-17},
  file = {C:\Users\raphi\Zotero\storage\I336RMC5\Badenes-Ribera et al. - 2016 - Misconceptions of the p-value among Chilean and Italian Academic Psychologists.pdf}
}

@article{beltagy_etal19,
  title = {{{SciBERT}}: {{A Pretrained Language Model}} for {{Scientific Text}}},
  shorttitle = {{{SciBERT}}},
  author = {Beltagy, Iz and Lo, Kyle and Cohan, Arman},
  year = {2019},
  publisher = {arXiv},
  doi = {10.48550/ARXIV.1903.10676},
  urldate = {2025-04-16},
  abstract = {Obtaining large-scale annotated data for NLP tasks in the scientific domain is challenging and expensive. We release SciBERT, a pretrained language model based on BERT (Devlin et al., 2018) to address the lack of high-quality, large-scale labeled scientific data. SciBERT leverages unsupervised pretraining on a large multi-domain corpus of scientific publications to improve performance on downstream scientific NLP tasks. We evaluate on a suite of tasks including sequence tagging, sentence classification and dependency parsing, with datasets from a variety of scientific domains. We demonstrate statistically significant improvements over BERT and achieve new state-of-the-art results on several of these tasks. The code and pretrained models are available at https://github.com/allenai/scibert/.},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords = {Computation and Language (cs.CL),FOS: Computer and information sciences}
}

@article{bernard_etal17,
  title = {`{{Sing Me}} a {{Song}} with {{Social Significance}}': {{The}} ({{Mis}}){{Use}} of {{Statistical Significance Testing}} in {{European Sociological Research}}},
  shorttitle = {`{{Sing Me}} a {{Song}} with {{Social Significance}}'},
  author = {Bernard, Fabrizio and Chakhaia, Lela and Leopold, Liliya},
  year = {2017},
  month = feb,
  journal = {European Sociological Review},
  volume = {33},
  number = {1},
  pages = {16--16},
  issn = {0266-7215, 1468-2672},
  doi = {10.1093/esr/jcx044},
  urldate = {2025-04-17},
  langid = {english},
  file = {C:\Users\raphi\Zotero\storage\LMCJDW58\Bernard et al. - 2017 - ‘Sing Me a Song with Social Significance’ The (Mis)Use of Statistical Significance Testing in Europ.pdf}
}

@article{brydges_gaeta19,
  title = {An {{Analysis}} of {{Nonsignificant Results}} in {{Audiology Using Bayes Factors}}},
  author = {Brydges, Christopher R. and Gaeta, Laura},
  year = {2019},
  month = dec,
  journal = {Journal of Speech, Language, and Hearing Research},
  volume = {62},
  number = {12},
  pages = {4544--4553},
  issn = {1092-4388, 1558-9102},
  doi = {10.1044/2019_JSLHR-H-19-0182},
  urldate = {2025-04-17},
  abstract = {Purpose                                Null hypothesis significance testing is commonly used in audiology research to determine the presence of an effect. Knowledge of study outcomes, including nonsignificant findings, is important for evidence-based practice. Nonsignificant                 p                 values obtained from null hypothesis significance testing cannot differentiate between true null effects or underpowered studies. Bayes factors (BFs) are a statistical technique that can distinguish between conclusive and inconclusive nonsignificant results, and quantify the strength of evidence in favor of 1 hypothesis over another. This study aimed to investigate the prevalence of BFs in nonsignificant results in audiology research and the strength of evidence in favor of the null hypothesis in these results.                                                        Method                                Nonsignificant results mentioned in abstracts of articles published in 2018 volumes of 4 prominent audiology journals were extracted (                 N                 = 108) and categorized based on whether BFs were calculated. BFs were calculated from nonsignificant                 t                 tests within this sample to determine how frequently the null hypothesis was strongly supported.                                                        Results                                Nonsignificant results were not directly tested with BFs in any study. Bayesian re-analysis of 93 nonsignificant                 t                 tests found that only 40.86\% of findings provided moderate evidence in favor of the null hypothesis, and none provided strong evidence.                                                        Conclusion               BFs are underutilized in audiology research, and a large proportion of null findings were deemed inconclusive when re-analyzed with BFs. Researchers are encouraged to use BFs to test the validity and strength of evidence of nonsignificant results and ensure that sufficient sample sizes are used so that conclusive findings (significant or not) are observed more frequently.                                         Supplemental Material                                https://osf.io/b4kc7/},
  langid = {english}
}

@misc{bucher_martini24,
  title = {Fine-{{Tuned}} '{{Small}}' {{LLMs}} ({{Still}}) {{Significantly Outperform Zero-Shot Generative AI Models}} in {{Text Classification}}},
  author = {Bucher, Martin Juan Jos{\'e} and Martini, Marco},
  year = {2024},
  month = aug,
  number = {arXiv:2406.08660},
  eprint = {2406.08660},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.08660},
  urldate = {2025-04-29},
  abstract = {Generative AI offers a simple, prompt-based alternative to fine-tuning smaller BERT-style LLMs for text classification tasks. This promises to eliminate the need for manually labeled training data and task-specific model training. However, it remains an open question whether tools like ChatGPT can deliver on this promise. In this paper, we show that smaller, fine-tuned LLMs (still) consistently and significantly outperform larger, zero-shot prompted models in text classification. We compare three major generative AI models (ChatGPT with GPT-3.5/GPT-4 and Claude Opus) with several fine-tuned LLMs across a diverse set of classification tasks (sentiment, approval/disapproval, emotions, party positions) and text categories (news, tweets, speeches). We find that fine-tuning with application-specific training data achieves superior performance in all cases. To make this approach more accessible to a broader audience, we provide an easy-to-use toolkit alongside this paper. Our toolkit, accompanied by non-technical step-by-step guidance, enables users to select and fine-tune BERT-like LLMs for any classification task with minimal technical and computational effort.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language}
}

@article{colquhoun14,
  title = {An Investigation of the False Discovery Rate and the Misinterpretation of {\emph{p}} -Values},
  author = {Colquhoun, David},
  year = {2014},
  month = nov,
  journal = {Royal Society Open Science},
  volume = {1},
  number = {3},
  pages = {140216},
  issn = {2054-5703},
  doi = {10.1098/rsos.140216},
  urldate = {2025-04-16},
  abstract = {If you use               p               =0.05 to suggest that you have made a discovery, you will be wrong at least 30\% of the time. If, as is often the case, experiments are underpowered, you will be wrong most of the time. This conclusion is demonstrated from several points of view. First, tree diagrams which show the close analogy with the screening test problem. Similar conclusions are drawn by repeated simulations of               t               -tests. These mimic what is done in real life, which makes the results more persuasive. The simulation method is used also to evaluate the extent to which effect sizes are over-estimated, especially in underpowered experiments. A script is supplied to allow the reader to do simulations themselves, with numbers appropriate for their own work. It is concluded that if you wish to keep your false discovery rate below 5\%, you need to use a three-sigma rule, or to insist on               p               {$\leq$}0.001. And               never               use the word `significant'.},
  langid = {english},
  file = {C:\Users\raphi\Zotero\storage\U4R2R63F\Colquhoun - 2014 - An investigation of the false discovery rate and the misinterpretation of p -values.pdf}
}

@misc{cummin_hussey24,
  title = {{{RegCheck}}. {{Compare}} Preregistrations with Papers. {{Instantly}}.},
  author = {Cummin, Jamie and Hussey, Ian},
  year = {2024},
  howpublished = {Available at https://regcheck.app/}
}

@article{cumming08,
  title = {Replication and {\emph{p}} {{Intervals}}: {\emph{p}} {{Values Predict}} the {{Future Only Vaguely}}, but {{Confidence Intervals Do Much Better}}},
  shorttitle = {Replication and {\emph{p}} {{Intervals}}},
  author = {Cumming, Geoff},
  year = {2008},
  month = jul,
  journal = {Perspectives on Psychological Science},
  volume = {3},
  number = {4},
  pages = {286--300},
  issn = {1745-6916, 1745-6924},
  doi = {10.1111/j.1745-6924.2008.00079.x},
  urldate = {2025-04-16},
  abstract = {Replication is fundamental to science, so statistical analysis should give information about replication. Because p values dominate statistical analysis in psychology, it is important to ask what p says about replication. The answer to this question is ``Surprisingly little.'' In one simulation of 25 repetitions of a typical experiment, p varied from {$<$}.001 to .76, thus illustrating that p is a very unreliable measure. This article shows that, if an initial experiment results in two-tailed p = .05, there is an 80\% chance the one-tailed p value from a replication will fall in the interval (.00008, .44), a 10\% chance that p {$<$}.00008, and fully a 10\% chance that p {$>$}.44. Remarkably, the interval---termed a p interval---is this wide however large the sample size. p is so unreliable and gives such dramatically vague information that it is a poor basis for inference. Confidence intervals, however, give much better information about replication. Researchers should minimize the role of p by using confidence intervals and model-fitting techniques and by adopting meta-analytic thinking.},
  copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
  langid = {english}
}

@article{cumming14,
  title = {The {{New Statistics}}: {{Why}} and {{How}}},
  shorttitle = {The {{New Statistics}}},
  author = {Cumming, Geoff},
  year = {2014},
  month = jan,
  journal = {Psychological Science},
  volume = {25},
  number = {1},
  pages = {7--29},
  issn = {0956-7976, 1467-9280},
  doi = {10.1177/0956797613504966},
  urldate = {2025-03-21},
  abstract = {We need to make substantial changes to how we conduct research. First, in response to heightened concern that our published research literature is incomplete and untrustworthy, we need new requirements to ensure research integrity. These include prespecification of studies whenever possible, avoidance of selection and other inappropriate data-analytic practices, complete reporting, and encouragement of replication. Second, in response to renewed recognition of the severe flaws of null-hypothesis significance testing (NHST), we need to shift from reliance on NHST to estimation and other preferred techniques. The new statistics refers to recommended practices, including estimation based on effect sizes, confidence intervals, and meta-analysis. The techniques are not new, but adopting them widely would be new for many researchers, as well as highly beneficial. This article explains why the new statistics are important and offers guidance for their use. It describes an eight-step new-statistics strategy for research with integrity, which starts with formulation of research questions in estimation terms, has no place for NHST, and is aimed at building a cumulative quantitative discipline.},
  langid = {english}
}

@misc{debruine_lakens25,
  title = {Papercheck: {{Check}} Scientific Papers for Best Practices.},
  author = {DeBruine, Lisa and Lakens, Dani{\"e}l},
  year = {2025},
  urldate = {2025-04-15},
  howpublished = {R package version 0.0.0.9033, available at https://scienceverse.github.io/papercheck/}
}

@inproceedings{devlin_etal19,
  title = {{{BERT}}: {{Pre-training}} of {{Deep Bidirectional Transformers}} for {{Language Understanding}}},
  booktitle = {Proceedings of the 2019 {{Conference}} of the {{North}}},
  author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  year = {2019},
  pages = {4171--4186},
  publisher = {Association for Computational Linguistics},
  address = {Minneapolis, Minnesota},
  doi = {10.18653/v1/N19-1423},
  urldate = {2025-04-16},
  langid = {english}
}

@article{edwards_etal63,
  title = {Bayesian Statistical Inference for Psychological Research.},
  author = {Edwards, Ward and Lindman, Harold and Savage, Leonard J.},
  year = {1963},
  month = may,
  journal = {Psychological Review},
  volume = {70},
  number = {3},
  pages = {193--242},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/h0044139},
  urldate = {2025-03-18},
  langid = {english}
}

@article{fidler_loftus09,
  title = {{Why Figures with Error Bars Should Replace \emph{p} Values: Some Conceptual Arguments and Empirical Demonstrations}},
  shorttitle = {{Why Figures with Error Bars Should Replace \emph{p} Values}},
  author = {Fidler, Fiona and Loftus, Geoffrey R.},
  year = {2009},
  month = jan,
  journal = {Zeitschrift f{\"u}r Psychologie / Journal of Psychology},
  volume = {217},
  number = {1},
  pages = {27--37},
  issn = {0044-3409},
  doi = {10.1027/0044-3409.217.1.27},
  urldate = {2025-03-11},
  abstract = {Null-hypothesis significance testing (NHST) is the primary means by which data are analyzed and conclusions made, particularly in the social sciences, but in other sciences as well (notably ecology and economics). Despite this supremacy however, numerous problems exist with NHST as a means of interpreting and understanding data. These problems have been articulated by various observers over the years, but are being taken seriously by researchers only slowly, if at all, as evidenced by the continuing emphasis on NHST in statistics classes, statistics textbooks, editorial policies and, of course, the day-to-day practices reported in empirical articles themselves ( Cumming et al., 2007 ). Over the past several decades, observers have suggested a simpler approach -- plotting the data with appropriate confidence intervals (CIs) around relevant sample statistics -- to supplement or take the place of hypothesis testing. This article addresses these issues.},
  langid = {ngerman},
  keywords = {CIs,prevalences},
  file = {C:\Users\raphi\OneDrive\RUB\1_Master\Masterarbeit\3_Writing\References\fidler_loftus09.pdf}
}

@article{finch_etal01,
  title = {Colloquium on {{Effect Sizes}}: The {{Roles}} of {{Editors}}, {{Textbook Authors}}, and the {{Publication Manual}}: {{Reporting}} of {{Statistical Inference}} in the {{Journal}} of {{Applied Psychology}}: {{Little Evidence}} of {{Reform}}},
  shorttitle = {Colloquium on {{Effect Sizes}}},
  author = {Finch, Sue and Cumming, Geoff and Thomason, Neil},
  year = {2001},
  month = apr,
  journal = {Educational and Psychological Measurement},
  volume = {61},
  number = {2},
  pages = {181--210},
  issn = {0013-1644, 1552-3888},
  doi = {10.1177/0013164401612001},
  urldate = {2025-03-10},
  abstract = {Reformers have long argued that misuse of Null Hypothesis Significance Testing (NHST) is widespread and damaging. The authors analyzed 150 articles from the Journal of Applied Psychology ( JAP) covering 1940 to 1999. They examined statistical reporting practices related to misconceptions about NHST, American Psychological Association (APA) guidelines, and reform recommendations. The analysis reveals (a) inconsistency in reporting alpha and p values, (b) the use of ambiguous language in describing NHST, (c) frequent acceptance of null hypotheses without consideration of power, (d) that power estimates are rarely reported, and (e) that confidence intervals were virtually never used. APA guidelines have been followed only selectively. Research methodology reported in JAP has increased greatly in sophistication over 60 years, but inference practices have shown remarkable stability. There is little sign that decades of cogent critiques by reformers had by 1999 led to changes in statistical reporting practices in JAP.},
  copyright = {https://journals.sagepub.com/page/policies/text-and-data-mining-license},
  langid = {english},
  keywords = {prevalences,previous research},
  file = {C:\Users\raphi\OneDrive\RUB\1_Master\Masterarbeit\3_Writing\References\finch_etal01.pdf}
}

@article{finch_etal01a,
  title = {Reporting of {{Statistical Inference}} in the {{Journal}} of {{Applied Psychology}}: {{Little Evidence}} of {{Reform}}},
  shorttitle = {Reporting of {{Statistical Inference}} in the {{Journal}} of {{Applied Psychology}}},
  author = {Finch, S. and Cumming, G. and Thomason, N.},
  year = {2001},
  month = apr,
  journal = {Educational and Psychological Measurement},
  volume = {61},
  number = {2},
  pages = {181--210},
  issn = {00000000, 00131644},
  doi = {10.1177/00131640121971167},
  urldate = {2025-04-17}
}

@article{goodman08,
  title = {A {{Dirty Dozen}}: {{Twelve P-Value Misconceptions}}},
  shorttitle = {A {{Dirty Dozen}}},
  author = {Goodman, Steven},
  year = {2008},
  month = jul,
  journal = {Seminars in Hematology},
  volume = {45},
  number = {3},
  pages = {135--140},
  issn = {00371963},
  doi = {10.1053/j.seminhematol.2008.04.003},
  urldate = {2025-03-10},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  keywords = {misinterpretations,theory},
  file = {C:\Users\raphi\OneDrive\RUB\1_Master\Masterarbeit\3_Writing\References\goodman08.pdf}
}

@article{goodman93,
  title = {P {{Values}}, {{Hypothesis Tests}}, and {{Likelihood}}: {{Implications}} for {{Epidemiology}} of a {{Neglected Historical Debate}}},
  shorttitle = {P {{Values}}, {{Hypothesis Tests}}, and {{Likelihood}}},
  author = {Goodman, Steven N.},
  year = {1993},
  month = mar,
  journal = {American Journal of Epidemiology},
  volume = {137},
  number = {5},
  pages = {485--496},
  issn = {1476-6256, 0002-9262},
  doi = {10.1093/oxfordjournals.aje.a116700},
  urldate = {2025-04-16},
  langid = {english}
}

@article{goodman99,
  title = {Toward {{Evidence-Based Medical Statistics}}. 1: {{The}} {{{\emph{P}}}} {{Value Fallacy}}},
  shorttitle = {Toward {{Evidence-Based Medical Statistics}}. 1},
  author = {Goodman, Steven N.},
  year = {1999},
  month = jun,
  journal = {Annals of Internal Medicine},
  volume = {130},
  number = {12},
  pages = {995--1004},
  issn = {0003-4819, 1539-3704},
  doi = {10.7326/0003-4819-130-12-199906150-00008},
  urldate = {2025-04-16},
  copyright = {https://www.acpjournals.org/journal/aim/text-and-data-mining},
  langid = {english}
}

@article{greenland_etal16,
  title = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power: A Guide to Misinterpretations},
  shorttitle = {Statistical Tests, {{P}} Values, Confidence Intervals, and Power},
  author = {Greenland, Sander and Senn, Stephen J. and Rothman, Kenneth J. and Carlin, John B. and Poole, Charles and Goodman, Steven N. and Altman, Douglas G.},
  year = {2016},
  month = apr,
  journal = {European Journal of Epidemiology},
  volume = {31},
  number = {4},
  pages = {337--350},
  issn = {0393-2990, 1573-7284},
  doi = {10.1007/s10654-016-0149-3},
  urldate = {2025-03-13},
  langid = {english},
  keywords = {misinterpretations,theory},
  file = {C\:\\Users\\raphi\\OneDrive\\RUB\\1_Master\\Masterarbeit\\3_Writing\\References\\greenland_etal16.pdf;C\:\\Users\\raphi\\Zotero\\storage\\8JRDAYWW\\Greenland et al_2016_Statistical tests, P values, confidence intervals, and power.pdf}
}

@article{greenland19,
  title = {Valid {{{\emph{P}}}} -{{Values Behave Exactly}} as {{They Should}}: {{Some Misleading Criticisms}} of {{{\emph{P}}}} -{{Values}} and {{Their Resolution With}} {{{\emph{S}}}} -{{Values}}},
  shorttitle = {Valid {{{\emph{P}}}} -{{Values Behave Exactly}} as {{They Should}}},
  author = {Greenland, Sander},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {106--114},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1529625},
  urldate = {2025-03-13},
  langid = {english},
  keywords = {defending p values},
  file = {C\:\\Users\\raphi\\OneDrive\\RUB\\1_Master\\Masterarbeit\\3_Writing\\References\\greenland19.pdf;C\:\\Users\\raphi\\Zotero\\storage\\HYV5F9T7\\Greenland_2019_Valid iP-i -Values Behave Exactly as They Should.pdf}
}

@article{gu_etal22,
  title = {Domain-{{Specific Language Model Pretraining}} for {{Biomedical Natural Language Processing}}},
  author = {Gu, Yu and Tinn, Robert and Cheng, Hao and Lucas, Michael and Usuyama, Naoto and Liu, Xiaodong and Naumann, Tristan and Gao, Jianfeng and Poon, Hoifung},
  year = {2022},
  month = jan,
  journal = {ACM Transactions on Computing for Healthcare},
  volume = {3},
  number = {1},
  pages = {1--23},
  issn = {2691-1957, 2637-8051},
  doi = {10.1145/3458754},
  urldate = {2025-04-16},
  abstract = {Pretraining large neural language models, such as BERT, has led to impressive gains on many natural language processing (NLP) tasks. However, most pretraining efforts focus on general domain corpora, such as newswire and Web. A prevailing assumption is that even domain-specific pretraining can benefit by starting from general-domain language models. In this article, we challenge this assumption by showing that for domains with abundant unlabeled text, such as biomedicine, pretraining language models from scratch results in substantial gains over continual pretraining of general-domain language models. To facilitate this investigation, we compile a comprehensive biomedical NLP benchmark from publicly available datasets. Our experiments show that domain-specific pretraining serves as a solid foundation for a wide range of biomedical NLP tasks, leading to new state-of-the-art results across the board. Further, in conducting a thorough evaluation of modeling choices, both for pretraining and task-specific fine-tuning, we discover that some common practices are unnecessary with BERT models, such as using complex tagging schemes in named entity recognition. To help accelerate research in biomedical NLP, we have released our state-of-the-art pretrained and task-specific models for the community, and created a leaderboard featuring our BLURB benchmark (short for Biomedical Language Understanding \& Reasoning Benchmark) at               https://aka.ms/BLURB               .},
  langid = {english},
  file = {C:\Users\raphi\Zotero\storage\YK5VFGC2\Gu et al. - 2022 - Domain-Specific Language Model Pretraining for Biomedical Natural Language Processing.pdf}
}

@article{haller_krauss02,
  title = {Misinterpretations of {{Significance}}: {{A}} Problem Students Share with Their Teachers?},
  shorttitle = {Misinterpretations of {{Significance}}},
  author = {Haller, H. and Krauss, Stefan},
  year = {2002},
  publisher = {Universit{\"a}t Regensburg},
  doi = {10.5283/EPUB.34338},
  urldate = {2025-04-17},
  abstract = {The use of significance tests in science has been debated from the invention of these tests until the present time. Apart from theoretical critiques on their appropriateness for evaluating scientific hypotheses, significance tests also receive criticism for inviting misinterpretations. We presented six common misinterpretations to psychologists who work in German universities and found out that they are still surprisingly widespread - even among instructors who teach statistics to psychology students. Although these mi-sinterpretations are well documented among students, until now there has been little research on pedagogical methods to remove them. Rather, they are considered "hard facts" that are impervious to correction. We discuss the roots of these misinterpreta-tions and propose a pedagogical concept to teach significance tests, which involves ex-plaining the meaning of statistical significance in an appropriate way.},
  keywords = {510 Mathematik,significance tests}
}

@article{harms_lakens18,
  title = {Making 'null Effects' Informative: Statistical Techniques and Inferential Frameworks},
  shorttitle = {Making 'null Effects' Informative},
  author = {Harms, Christopher and Lakens, Dani{\"e}l},
  year = {2018},
  journal = {Journal of Clinical and Translational Research},
  issn = {2424810X},
  doi = {10.18053/jctres.03.2017S2.007},
  urldate = {2025-03-20},
  file = {C:\Users\raphi\Zotero\storage\DTE6ILEL\2018_Making 'null effects' informative.pdf}
}

@misc{heathers_etal18,
  title = {Recovering Data from Summary Statistics: {{Sample Parameter Reconstruction}} via {{Iterative TEchniques}} ({{SPRITE}})},
  shorttitle = {Recovering Data from Summary Statistics},
  author = {Heathers, James A and Anaya, Jordan and Van Der Zee, Tim and Brown, Nicholas Jl},
  year = {2018},
  month = may,
  doi = {10.7287/peerj.preprints.26968v1},
  urldate = {2025-04-26},
  abstract = {Scientific publications have not traditionally been accompanied by data, either during the peer review process or when published. Concern has arisen that the literature in many fields may contain inaccuracies or errors that cannot be detected without inspecting the original data. Here, we introduce SPRITE (Sample Parameter Reconstruction via Interative TEchniques), a heuristic method for reconstructing plausible samples from descriptive statistics of granular data, allowing reviewers, editors, readers, and future researchers to gain insights into the possible distributions of item values in the original data set. This paper presents the principles of operation of SPRITE, as well as worked examples of its practical use for error detection in real published work. Full source code for three software implementations of SPRITE (in MATLAB, R, and Python) and two web-based implementations requiring no local installation (1, 2) are available for readers.},
  copyright = {http://creativecommons.org/licenses/by/4.0/},
  langid = {english}
}

@article{hoekstra_etal06,
  title = {Probability as Certainty: {{Dichotomous}} Thinking and the Misuse of p Values},
  shorttitle = {Probability as Certainty},
  author = {Hoekstra, Rink and Finch, Sue and Kiers, Henk A. L. and Johnson, Addie},
  year = {2006},
  month = dec,
  journal = {Psychonomic Bulletin \& Review},
  volume = {13},
  number = {6},
  pages = {1033--1037},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/BF03213921},
  urldate = {2025-03-10},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  keywords = {prevalences,previous research},
  file = {C\:\\Users\\raphi\\OneDrive\\RUB\\1_Master\\Masterarbeit\\3_Writing\\References\\hoekstra_etal06.pdf;C\:\\Users\\raphi\\Zotero\\storage\\MPPCA4JY\\Hoekstra et al_2006_Probability as certainty.pdf}
}

@article{hoekstra_etal14,
  title = {Robust Misinterpretation of Confidence Intervals},
  author = {Hoekstra, Rink and Morey, Richard D. and Rouder, Jeffrey N. and Wagenmakers, Eric-Jan},
  year = {2014},
  month = oct,
  journal = {Psychonomic Bulletin \& Review},
  volume = {21},
  number = {5},
  pages = {1157--1164},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-013-0572-3},
  urldate = {2025-03-11},
  langid = {english},
  keywords = {alternatives,CIs},
  file = {C:\Users\raphi\OneDrive\RUB\1_Master\Masterarbeit\3_Writing\References\hoekstra_etal14.pdf}
}

@misc{isager_fitzgerald24,
  title = {Three-{{Sided Testing}} to {{Establish Practical Significance}}: {{A Tutorial}}},
  shorttitle = {Three-{{Sided Testing}} to {{Establish Practical Significance}}},
  author = {Isager, Peder Mortvedt and Fitzgerald, Jack},
  year = {2024},
  month = dec,
  doi = {10.31234/osf.io/8y925},
  urldate = {2025-03-10},
  abstract = {Researchers may want to know whether an observed statistical relationship is either meaningfully negative, meaningfully positive, or small enough to be considered practically equivalent to zero. Such a question can not be addressed with standard null hypothesis significance testing, nor with standard equivalence testing. Three-sided testing (TST) is a procedure to address such questions, by simultaneously testing whetheran estimated relationship is significantly below, within, or above predetermined smallest effect sizes of interest. TST is a natural extension of the standard two one-sided test for equivalence (TOST). TST offers a more comprehensive decision framework than TOST with no penalty to error rates or statistical power. In this paper, we give a non-technical introduction to TST, provide commands for conducting TST in both Rand Jamovi, and provide a Shiny app for easy implementation. Whenever a meaningful smallest effect size of interest can be specified, TST should be combined with null hypothesis significance testing as the default frequentist testing procedure.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  keywords = {alternatives},
  file = {C:\Users\raphi\OneDrive\RUB\1_Master\Masterarbeit\3_Writing\References\isager_fitzgerald24.pdf}
}

@article{lakens_etal18,
  title = {Justify Your Alpha},
  author = {Lakens, Dani{\"e}l and Adolfi, Federico G. and Albers, Casper J. and Anvari, Farid and Apps, Matthew A. J. and Argamon, Shlomo E. and Baguley, Thom and Becker, Raymond B. and Benning, Stephen D. and Bradford, Daniel E. and Buchanan, Erin M. and Caldwell, Aaron R. and Van Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln J. and Collins, Gary S. and Crook, Zander and Cross, Emily S. and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel J. and Earp, Brian D. and Feist, Michele I. and Ferrell, Jason D. and Field, James G. and Fox, Nicholas W. and Friesen, Amanda and Gomes, Caio and {Gonzalez-Marquez}, Monica and Grange, James A. and Grieve, Andrew P. and Guggenberger, Robert and Grist, James and Van Harmelen, Anne-Laura and Hasselman, Fred and Hochard, Kevin D. and Hoffarth, Mark R. and Holmes, Nicholas P. and Ingre, Michael and Isager, Peder M. and Isotalus, Hanna K. and Johansson, Christer and Juszczyk, Konrad and Kenny, David A. and Khalil, Ahmed A. and Konat, Barbara and Lao, Junpeng and Larsen, Erik Gahner and Lodder, Gerine M. A. and Lukavsk{\'y}, Ji{\v r}{\'i} and Madan, Christopher R. and Manheim, David and Martin, Stephen R. and Martin, Andrea E. and Mayo, Deborah G. and McCarthy, Randy J. and McConway, Kevin and McFarland, Colin and Nio, Amanda Q. X. and Nilsonne, Gustav and De Oliveira, Cilene Lino and De Xivry, Jean-Jacques Orban and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly A. and Sakon, John J. and Saribay, S. Adil and Schneider, Iris K. and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel G. and Smits, Tim and Spies, Jeffrey R. and Sreekumar, Vishnu and Steltenpohl, Crystal N. and Stenhouse, Neil and {\'S}wi{\k a}tkowski, Wojciech and Vadillo, Miguel A. and Van Assen, Marcel A. L. M. and Williams, Matt N. and Williams, Samantha E. and Williams, Donald R. and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf A.},
  year = {2018},
  month = feb,
  journal = {Nature Human Behaviour},
  volume = {2},
  number = {3},
  pages = {168--171},
  issn = {2397-3374},
  doi = {10.1038/s41562-018-0311-x},
  urldate = {2025-03-13},
  langid = {english},
  file = {C:\Users\raphi\OneDrive\RUB\1_Master\Masterarbeit\3_Writing\References\lakens_etal18a.pdf}
}

@article{lakens_etal18a,
  title = {Equivalence {{Testing}} for {{Psychological Research}}: {{A Tutorial}}},
  shorttitle = {Equivalence {{Testing}} for {{Psychological Research}}},
  author = {Lakens, Dani{\"e}l and Scheel, Anne M. and Isager, Peder M.},
  year = {2018},
  month = jun,
  journal = {Advances in Methods and Practices in Psychological Science},
  volume = {1},
  number = {2},
  pages = {259--269},
  issn = {2515-2459, 2515-2467},
  doi = {10.1177/2515245918770963},
  urldate = {2025-03-10},
  abstract = {Psychologists must be able to test both for the presence of an effect and for the absence of an effect. In addition to testing against zero, researchers can use the two one-sided tests (TOST) procedure to test for equivalence and reject the presence of a smallest effect size of interest (SESOI). The TOST procedure can be used to determine if an observed effect is surprisingly small, given that a true effect at least as extreme as the SESOI exists. We explain a range of approaches to determine the SESOI in psychological science and provide detailed examples of how equivalence tests should be performed and reported. Equivalence tests are an important extension of the statistical tools psychologists currently use and enable researchers to falsify predictions about the presence, and declare the absence, of meaningful effects.},
  langid = {english},
  keywords = {alternatives},
  file = {C\:\\Users\\raphi\\OneDrive\\RUB\\1_Master\\Masterarbeit\\3_Writing\\References\\lakens_etal18.pdf;C\:\\Users\\raphi\\Zotero\\storage\\586YQN5D\\Lakens et al_2018_Equivalence Testing for Psychological Research.pdf}
}

@article{lakens21,
  title = {The {{Practical Alternative}} to the {\emph{p}} {{Value Is}} the {{Correctly Used}} {\emph{p}} {{Value}}},
  author = {Lakens, Dani{\"e}l},
  year = {2021},
  month = may,
  journal = {Perspectives on Psychological Science},
  volume = {16},
  number = {3},
  pages = {639--648},
  issn = {1745-6916, 1745-6924},
  doi = {10.1177/1745691620958012},
  urldate = {2025-03-10},
  abstract = {Because of the strong overreliance on p values in the scientific literature, some researchers have argued that we need to move beyond p values and embrace practical alternatives. When proposing alternatives to p values statisticians often commit the ``statistician's fallacy,'' whereby they declare which statistic researchers really ``want to know.'' Instead of telling researchers what they want to know, statisticians should teach researchers which questions they can ask. In some situations, the answer to the question they are most interested in will be the p value. As long as null-hypothesis tests have been criticized, researchers have suggested including minimum-effect tests and equivalence tests in our statistical toolbox, and these tests have the potential to greatly improve the questions researchers ask. If anyone believes p values affect the quality of scientific research, preventing the misinterpretation of p values by developing better evidence-based education and user-centered statistical software should be a top priority. Polarized discussions about which statistic scientists should use has distracted us from examining more important questions, such as asking researchers what they want to know when they conduct scientific research. Before we can improve our statistical inferences, we need to improve our statistical questions.},
  langid = {english},
  keywords = {alternatives,defending p values,theory},
  file = {C\:\\Users\\raphi\\OneDrive\\RUB\\1_Master\\Masterarbeit\\3_Writing\\References\\lakens21.pdf;C\:\\Users\\raphi\\Zotero\\storage\\XDQNHT4F\\Lakens_2021_The Practical Alternative to the ip-i Value Is the Correctly Used ip-i.pdf}
}

@book{lakens24a,
  title = {Improving {{Your Statistical Inferences}}},
  author = {Lakens, Dani{\"e}l},
  year = {2024},
  month = jun
}

@article{lyu_etal20,
  title = {Beyond Psychology: Prevalence of {\emph{p}} Value and Confidence Interval Misinterpretation across Different Fields},
  shorttitle = {Beyond Psychology},
  author = {Lyu, Xiao-Kang and Xu, Yuepei and Zhao, Xiao-Fan and Zuo, Xi-Nian and Hu, Chuan-Peng},
  year = {2020},
  month = jan,
  journal = {Journal of Pacific Rim Psychology},
  volume = {14},
  pages = {e6},
  issn = {1834-4909, 1834-4909},
  doi = {10.1017/prp.2019.28},
  urldate = {2025-04-17},
  abstract = {P values and confidence intervals (CIs) are the most widely used statistical indices in scientific literature. Several surveys have revealed that these two indices are generally misunderstood. However, existing surveys on this subject fall under psychology and biomedical research, and data from other disciplines are rare. Moreover, the confidence of researchers when constructing judgments remains unclear. To fill this research gap, we surveyed 1,479 researchers and students from different fields in China. Results reveal that for significant (i.e., p {$<$} .05, CI does not include zero) and non-significant (i.e., p {$>$} .05, CI includes zero) conditions, most respondents, regardless of academic degrees, research fields and stages of career, could not interpret p values and CIs accurately. Moreover, the majority were confident about their (inaccurate) judgements (see osf.io/mcu9q/ for raw data, materials, and supplementary analyses). Therefore, as misinterpretations of p values and CIs prevail in the whole scientific community, there is a need for better statistical training in science.},
  langid = {english}
}

@article{mcshane_etal19,
  title = {Abandon {{Statistical Significance}}},
  author = {McShane, Blakeley B. and Gal, David and Gelman, Andrew and Robert, Christian and Tackett, Jennifer L.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {235--245},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2018.1527253},
  urldate = {2025-04-16},
  langid = {english},
  file = {C:\Users\raphi\Zotero\storage\TDEASG7I\McShane et al. - 2019 - Abandon Statistical Significance.pdf}
}

@article{murphy_etal25,
  title = {Nonsignificance Misinterpreted as an Effect's Absence in Psychology: Prevalence and Temporal Analyses},
  shorttitle = {Nonsignificance Misinterpreted as an Effect's Absence in Psychology},
  author = {Murphy, Stephen Lee and Merz, Raphael and Reimann, Linda-Elisabeth and Fern{\'a}ndez, Aurelio},
  year = {2025},
  month = mar,
  journal = {Royal Society Open Science},
  volume = {12},
  number = {3},
  pages = {242167},
  issn = {2054-5703},
  doi = {10.1098/rsos.242167},
  urldate = {2025-03-19},
  abstract = {Nonsignificant findings in psychological research are frequently misinterpreted as reflecting the effect's absence. However, this issue's exact prevalence remains unclear, as does whether this issue is getting better or worse. In this pre-registered study, we sought to answer these questions by examining the discussion sections of 599 articles published across 10 psychology journals and three time points (2009, 2015 and 2021), and coding whether a nonsignificant finding was interpreted in such a way as to suggest the effect does not exist. Our models indicate that between 76\% and 85\% of psychology articles published between 2009 and 2021 that discussed a nonsignificant finding misinterpreted nonsignificance as reflecting no effect. It is likely between 54\% and 62\% of articles over this time period claimed explicitly that this meant no effect on the population of interest. Our findings also indicate that only between 4\% and 8\% of articles explicitly discussed the possibility that the nonsignificant effect may exist but could not be found. Differences in prevalence rates over time were nonsignificant. Collectively, our findings indicate this interpretative error is a major problem in psychology. We call on stakeholders with an interest in improving psychological science to prioritize tackling it.},
  langid = {english},
  file = {C:\Users\raphi\OneDrive\RUB\1_Master\Masterarbeit\3_Writing\References\murphy_etal25.pdf}
}

@misc{nuijten_epskamp24,
  title = {Statcheck: {{Extract}} Statistics from Articles and Recompute p-Values. {{R}} Package Version 1.5.0.},
  author = {Nuijten, Mich{\`e}le B. and Epskamp, Sacha},
  year = {2024},
  urldate = {2025-04-26},
  howpublished = {Web implementation at https://statcheck.io}
}

@article{schervish96,
  title = {{\emph{P}} {{Values}}: {{What They}} Are and {{What They}} Are {{Not}}},
  shorttitle = {{\emph{P}} {{Values}}},
  author = {Schervish, Mark J.},
  year = {1996},
  month = aug,
  journal = {The American Statistician},
  volume = {50},
  number = {3},
  pages = {203--206},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.1996.10474380},
  urldate = {2025-03-13},
  langid = {english},
  file = {C:\Users\raphi\OneDrive\RUB\1_Master\Masterarbeit\3_Writing\References\schervish96.pdf}
}

@article{senn01,
  title = {Two Cheers for {{P-values}}?},
  author = {Senn, S},
  year = {2001},
  month = mar,
  journal = {Journal of Epidemiology and Biostatistics},
  volume = {6},
  number = {2},
  pages = {193--204},
  issn = {1359-5229},
  doi = {10.1080/135952201753172953},
  urldate = {2025-04-16}
}

@misc{stillman19,
  title = {Retracted Item Notifications with {{Retraction Watch}} Integration},
  author = {Stillman, Dan},
  year = {2019},
  month = jun,
  urldate = {2025-04-26},
  howpublished = {https://www.zotero.org/blog/retracted-item-notifications/},
  langid = {english}
}

@article{thompson04,
  title = {The ``Significance'' Crisis in Psychology and Education},
  author = {Thompson, Bruce},
  year = {2004},
  month = nov,
  journal = {The Journal of Socio-Economics},
  volume = {33},
  number = {5},
  pages = {607--613},
  issn = {10535357},
  doi = {10.1016/j.socec.2004.09.034},
  urldate = {2025-04-29},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english}
}

@article{vacha-haase_ness99,
  title = {Statistical Significance Testing as It Relates to Practice: {{Use}} within {{Professional Psychology}}: {{Research}} and {{Practice}}.},
  shorttitle = {Statistical Significance Testing as It Relates to Practice},
  author = {{Vacha-Haase}, Tammi and Ness, Carin M.},
  year = {1999},
  month = feb,
  journal = {Professional Psychology: Research and Practice},
  volume = {30},
  number = {1},
  pages = {104--105},
  issn = {1939-1323, 0735-7028},
  doi = {10.1037/0735-7028.30.1.104},
  urldate = {2025-04-17},
  langid = {english}
}

@misc{vandongen_vangrootel21,
  title = {Overview on the {{Null Hypothesis Significance Test}}},
  author = {Van Dongen, Noah N'Djaye Nikolai and Van Grootel, Leonie},
  year = {2021},
  month = jan,
  doi = {10.31234/osf.io/hwk4n},
  urldate = {2025-03-13},
  abstract = {For decades, waxing and waning, there has been an ongoing debate on the values and problems of the ubiquitously used null hypothesis significance test (NHST). With the start of the replication crisis, this debate has flared-up once again, especially in the psychology and psychological methods literature. Arguing for or against the NHST method usually takes place in essay and opinion pieces that cover some, but not all the qualities and problems of the method. The NHST literature landscape is vast, a clear overview is lacking, and participants in the debate seem to be talking past one another. To contribute to a resolution, we conducted a systematic review on essay literature concerning NHST published in psychology and psychological methods journals between 2011 and 2018. We extracted all arguments in defense of (20) and against (70) NHST, and we extracted the solutions (33) that were proposed to remedy (some of) the perceived problems of NHST. Unfiltered, these 123 items form a landscape that is prohibitively difficult to keep in one's sights. Our contribution to the resolution of the NHST debate is twofold. 1) We performed a thematic synthesis of the arguments and solutions that carves the landscape in a framework of three zones: mild, moderate, and critical. This reduction summarizes groups of arguments and solutions, thus offering a manageable overview of NHST's qualities, problems, and solutions.  2) We provide the data on the arguments and solutions as a resource for those who will carry-on the debate and/or study the use of NHST.},
  file = {C\:\\Users\\raphi\\OneDrive\\RUB\\1_Master\\Masterarbeit\\3_Writing\\References\\vandongen_vangrootel21.pdf;C\:\\Users\\raphi\\Zotero\\storage\\P94GBUK4\\Van Dongen_Van Grootel_2021_Overview on the Null Hypothesis Significance Test.pdf}
}

@article{wagenmakers_etal18,
  title = {Bayesian Inference for Psychology. {{Part I}}: {{Theoretical}} Advantages and Practical Ramifications},
  shorttitle = {Bayesian Inference for Psychology. {{Part I}}},
  author = {Wagenmakers, Eric-Jan and Marsman, Maarten and Jamil, Tahira and Ly, Alexander and Verhagen, Josine and Love, Jonathon and Selker, Ravi and Gronau, Quentin F. and {\v S}m{\'i}ra, Martin and Epskamp, Sacha and Matzke, Dora and Rouder, Jeffrey N. and Morey, Richard D.},
  year = {2018},
  month = feb,
  journal = {Psychonomic Bulletin \& Review},
  volume = {25},
  number = {1},
  pages = {35--57},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/s13423-017-1343-3},
  urldate = {2025-03-18},
  langid = {english},
  file = {C\:\\Users\\raphi\\OneDrive\\RUB\\1_Master\\Masterarbeit\\3_Writing\\References\\wagenmakers_etal18.pdf;C\:\\Users\\raphi\\Zotero\\storage\\RTHTINRS\\Wagenmakers et al_2018_Bayesian inference for psychology.pdf}
}

@article{wagenmakers07,
  title = {A Practical Solution to the Pervasive Problems of {\emph{p}} Values},
  author = {Wagenmakers, Eric-Jan},
  year = {2007},
  month = oct,
  journal = {Psychonomic Bulletin \& Review},
  volume = {14},
  number = {5},
  pages = {779--804},
  issn = {1069-9384, 1531-5320},
  doi = {10.3758/BF03194105},
  urldate = {2025-03-18},
  copyright = {http://www.springer.com/tdm},
  langid = {english},
  file = {C\:\\Users\\raphi\\OneDrive\\RUB\\1_Master\\Masterarbeit\\3_Writing\\References\\wagenmakers07.pdf;C\:\\Users\\raphi\\Zotero\\storage\\7WM7DFXE\\Wagenmakers_2007_A practical solution to the pervasive problems ofp values.pdf}
}

@article{wasserstein_etal19,
  title = {Moving to a {{World Beyond}} `` {\emph{p}} {$<$} 0.05''},
  author = {Wasserstein, Ronald L. and Schirm, Allen L. and Lazar, Nicole A.},
  year = {2019},
  month = mar,
  journal = {The American Statistician},
  volume = {73},
  number = {sup1},
  pages = {1--19},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2019.1583913},
  urldate = {2025-04-17},
  langid = {english}
}

@article{wasserstein_lazar16,
  title = {The {{ASA Statement}} on {\emph{p}} -{{Values}}: {{Context}}, {{Process}}, and {{Purpose}}},
  shorttitle = {The {{ASA Statement}} on {\emph{p}} -{{Values}}},
  author = {Wasserstein, Ronald L. and Lazar, Nicole A.},
  year = {2016},
  month = apr,
  journal = {The American Statistician},
  volume = {70},
  number = {2},
  pages = {129--133},
  issn = {0003-1305, 1537-2731},
  doi = {10.1080/00031305.2016.1154108},
  urldate = {2025-04-16},
  langid = {english}
}
